{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:3em;\">Ch5: The Mechanics of Learning\n",
    "</span>\n",
    "\n",
    "# 5.1 A timeless lesson in modeling\n",
    "\n",
    " We can argue that learning from data presumes the underlying model is not engineered to solve a specific problem and is instead capable of approximating a much wider family of functions\n",
    " \n",
    "  In this book, we’re interested in models that are not engineered for solving a specific narrow task, but that can be automatically adapted to specialize themselves for\n",
    "any one of many similar tasks using input and output pairs—in other words, general\n",
    "models trained on data relevant to the specific task at hand. In particular, PyTorch is\n",
    "designed to make it easy to create models for which the derivatives of the fitting error,\n",
    "with respect to the parameters, can be expressed analytically\n",
    "\n",
    " In this book, we’re interested in models that are not engineered for solving a specific narrow task, but that can be automatically adapted to specialize themselves for\n",
    "any one of many similar tasks using input and output pairs—in other words, general\n",
    "models trained on data relevant to the specific task at hand. In particular, PyTorch is\n",
    "designed to make it easy to create models for which the derivatives of the fitting error,\n",
    "with respect to the parameters, can be expressed analytically.  \n",
    "\n",
    "In order to make sure we get the key concepts right, we’ll start with a model\n",
    "that is a lot simpler than a deep neural network\n",
    "\n",
    "# 5.2 Learning is just parameter estimation\n",
    "\n",
    "Given input data and the corresponding desired outputs (ground truth), as\n",
    "well as initial values for the weights, the model is fed input data (forward pass), and a\n",
    "measure of the error is evaluated by comparing the resulting outputs to the ground\n",
    "truth. In order to optimize the parameter of the model—its weights—the change in\n",
    "the error following a unit change in weights (that is, the gradient of the error with\n",
    "respect to the parameters) is computed using the chain rule for the derivative of a\n",
    "composite function (backward pass). The value of the weights is then updated in the\n",
    "direction that leads to a decrease in the error\n",
    "\n",
    "When we start, we’ll be doing everything by hand,\n",
    "but by the end of the chapter we’ll be letting PyTorch do all the heavy lifting for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5000, 14.0000, 15.0000, 28.0000, 11.0000,  8.0000,  3.0000, -4.0000,\n",
       "         6.0000, 13.0000, 21.0000])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We will start by using temperature data \n",
    "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x122b89d50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#quick \n",
    "plt.scatter(t_c,t_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The\n",
    "two may be linearly related—that is, multiplying t_u by a factor and adding a constant,\n",
    "we may get the temperature in Celsius:\n",
    "<b>t_c = w * t_u + b</b>\n",
    "\n",
    " Let’s flesh it out again: we have a model with some unknown parameters, and we\n",
    "need to estimate those parameters so that the error between predicted outputs and\n",
    "measured values is as low as possible. We notice that we still need to exactly define a\n",
    "measure of the error. Such a measure, which we refer to as the loss function, should be\n",
    "high if the error is high and should ideally be as low as possible for a perfect match.\n",
    "Our optimization process should therefore aim at finding w and b so that the loss\n",
    "function is at a minimum.\n",
    "\n",
    "\n",
    "# 5.3 Less loss is what we want\n",
    "A loss function (or cost function) is a function that computes a single numerical value\n",
    "that the learning process will attempt to minimize. \n",
    "\n",
    "In our case,\n",
    "that would be the difference between the predicted temperatures t_p output by our\n",
    "model and the actual measurements: t_p – t\n",
    "\n",
    "<b> Mean Squared Error</b> is a common loss function for regression.\n",
    "\n",
    "MSE has a clear minimum at 0 and grows monotically as the prediction moves away from the true value in both directions. Because steepness of the growth also monotically increases away from the minimum, it is a <b>convex</b> function. Since our model is linear, the loss as a function of w and b are also convex\n",
    "\n",
    "\n",
    " Cases where the loss is a convex function of the model parameters are usually great to deal with because we can find a minimum very efficiently through specialized algorithms. However, we will instead use less powerful but more\n",
    "generally applicable methods in this chapter. We do so because for the deep neural networks we are ultimately interested in, the loss is not a convex function of the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For our two loss functions <b>|t_p – t_c| and (t_p – t_c)^2</b>, as shown in figure 5.4,\n",
    "we notice that the square of the differences behaves more nicely around the minimum: the derivative of the error-squared loss with respect to t_p is zero when t_p\n",
    "equals t_c. The absolute value, on the other hand, has an undefined derivative right\n",
    "where we’d like to converge. This is less of an issue in practice than it looks like, but\n",
    "we’ll stick to the square of differences for the time being.\n",
    "\n",
    "## 5.3.1 From problem back to PyTorch\n",
    "\n",
    " Now we need to set the learning process in motion and feed it actual data. \n",
    " We’ve already created our data tensors, so now let’s write out the model as a\n",
    "Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re expecting t_u, w, and b to be the input tensor, weight parameter, and bias\n",
    "parameter, respectively. In our model, the parameters will be PyTorch scalars (aka zero-dimensional tensors), and the product operation will use broadcasting to yield\n",
    "the returned tensors.\n",
    "\n",
    "Now lets define loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p,t_c):\n",
    "    '''\n",
    "    Mean Squared Loss\n",
    "    '''\n",
    "    squared_dffs = (t_p - t_c)**2\n",
    "    return squared_dffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,\n",
       "        48.4000, 60.4000, 68.4000])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now lets initialize w and b and create the model\n",
    "w = torch.ones(())\n",
    "b = torch.zeros(())\n",
    "\n",
    "t_p = model(t_u, w, b)\n",
    "t_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8846)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loss\n",
    "loss = loss_fn(t_p, t_c)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Broadcasting:</b>\n",
    "\n",
    "\n",
    "We mentioned broadcasting in chapter 3, and we promised to look at it more carefully\n",
    "when we need it. In our example, we have two scalars (zero-dimensional tensors) w\n",
    "and b, and we multiply them with and add them to vectors (one-dimensional tensors)\n",
    "of length b.\n",
    "Usually—and in early versions of PyTorch, too—we can only use element-wise binary\n",
    "operations such as addition, subtraction, multiplication, and division for arguments\n",
    "of the same shape. The entries in matching positions in each of the tensors will be\n",
    "used to calculate the corresponding entry in the result tensor.\n",
    "\n",
    "\n",
    "Broadcasting, which is popular in NumPy and adapted by PyTorch, relaxes this assumption for most binary operations. It uses the following rules to match tensor elements:\n",
    "- For each index dimension, counted from the back, if one of the operands is\n",
    "size 1 in that dimension, PyTorch will use the single entry along this dimension with each of the entries in the other tensor along this dimension.\n",
    "\n",
    "\n",
    "- If both sizes are greater than 1, they must be the same, and natural matching\n",
    "is used.\n",
    "\n",
    "\n",
    "- If one of the tensors has more index dimensions than the other, the entirety\n",
    "of the other tensor will be used for each entry along these dimensions.\n",
    "\n",
    "This sounds complicated (and it can be error-prone if we don’t pay close attention, which\n",
    "is why we have named the tensor dimensions as shown in section 3.4), but usually,\n",
    "we can either write down the tensor dimensions to see what happens or picture what\n",
    "happens by using space dimensions to show the broadcasting, as in the following figure.\n",
    "\n",
    "<b>Note:</b> This is for element-wise multiplication below not matrix multiplication(dot product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: x: torch.Size([]) y: torch.Size([3, 1])\n",
      "z: torch.Size([1, 3]), a: torch.Size([2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(())\n",
    "y = torch.ones(3,1)\n",
    "z = torch.ones(1,3)\n",
    "a = torch.ones(2,1,1)\n",
    "\n",
    "print(f'shapes: x: {x.shape} y: {y.shape}')\n",
    "print(f\"z: {z.shape}, a: {a.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x * y: torch.Size([3, 1])\n",
      "y * z: torch.Size([3, 3])\n",
      "y * z * a: torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"x * y:\", (x * y).shape)\n",
    "print(\"y * z:\", (y * z).shape)\n",
    "print(\"y * z * a:\", (y * z * a).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 Down along the gradient\n",
    "\n",
    "We’ll optimize the loss function with respect to the parameters using the gradient\n",
    "descent algorithm.  Gradient descent is actually a very simple idea, and it scales up surprisingly well to large neural network models with millions of parameters.\n",
    "\n",
    "## 5.4.1 Decreasing loss\n",
    "\n",
    "The idea to to compute the rate of change of the loss with respect to each parameter, and modify each parameter in the direction of decreasing loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.1\n",
    "\n",
    "#this backwards slash allows us to write on the next line, its called line continutation character\n",
    "loss_rate_of_change_w = \\\n",
    "(loss_fn(model(t_u, w + delta, b), t_c) -\n",
    "loss_fn(model(t_u, w - delta, b), t_c)) / (2.0 * delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4517.2979)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_rate_of_change_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is saying that in the neighborhood of the current values of w and b, a unit\n",
    "increase in w leads to some change in the loss. If the change is negative, then we need\n",
    "to increase w to minimize the loss, whereas if the change is positive, we need to\n",
    "decrease w\n",
    "\n",
    "It is also wise to\n",
    "change the parameters slowly in general, because the rate of change could be dramatically different at a distance from the neighborhood of the current w value. Therefore,\n",
    "we typically should scale the rate of change by a small factor. This scaling factor has\n",
    "many names; the one we use in machine learning is <b>learning_rate</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "w = w - learning_rate* loss_rate_of_change_w\n",
    "\n",
    "#Can do the same with b:\n",
    "loss_rate_of_change_b = \\\n",
    "(loss_fn(model(t_u, w, b + delta), t_c) -\n",
    "loss_fn(model(t_u, w, b - delta), t_c)) / (2.0 * delta)\n",
    "\n",
    "b=b- learning_rate * loss_rate_of_change_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This represents the basic parameter-update step for gradient descent. By reiterating\n",
    "these evaluations (and provided we choose a small enough learning rate), we will\n",
    "converge to an optimal value of the parameters for which the loss computed on the\n",
    "given data is minimal.\n",
    "\n",
    "## 5.4.2 Getting analytical\n",
    "\n",
    "In a model with two or more parameters like the one we’re\n",
    "dealing with, we compute the individual derivatives of the loss with respect to each\n",
    "parameter and put them in a vector of derivatives: the gradient.\n",
    "\n",
    "In order to compute the derivative of the loss with respect to a parameter, we can\n",
    "apply the chain rule and compute the derivative of the loss with respect to its input\n",
    "(which is the output of the model), times the derivative of the model with respect to\n",
    "the parameter:\n",
    "\n",
    "\n",
    "\n",
    "dloss_fn / d w = (dloss_fn / d t_p) * (d t_p / d w)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p,t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()\n",
    "\n",
    "#Remembering that d x^2 / d x = 2 x, we get\n",
    "def dloss_fn(t_p, t_c):\n",
    "    dsg_diffs = 2 * (t_p - t_c) / t_p.size(0)\n",
    "    return dsg_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the derivatives to the model from earlier get\n",
    "def dmodel_dw(t_u, w, b):\n",
    "    return t_u\n",
    "def dmodel_db(t_u,w,b):\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>DEFINING THE GRADIENT FUNCTION: </b>\n",
    "Putting all of this together, the function returning the gradient of the loss with respect\n",
    "to w and b is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fn(t_u, t_c, t_p, w, b):\n",
    "    dloss_dtp = dloss_fn(t_p,t_c)\n",
    "    dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b)\n",
    "    dloss_db = dloss_dtp * dmodel_db(t_u,w,b)\n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.3 Iterating to fit the model\n",
    "\n",
    "We now have everything in place to optimize our parameters. Starting from a tentative\n",
    "value for a parameter, we can iteratively apply updates to it for a fixed number of iterations, or until w and b stop changing. There are several stopping criteria; for now,\n",
    "we’ll stick to a fixed number of iterations.\n",
    "\n",
    "<b>THE TRAINING LOOP: </b>\n",
    "Since we’re at it, let’s introduce another piece of terminology. We call a training iteration during which we update the parameters for all of our training samples an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The complete training loop looks like this \n",
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c, print_params=True):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w, b = params\n",
    "        t_p = model(t_u, w, b) # forward pass\n",
    "        loss = loss_fn(t_p, t_c) \n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b) # Backward pass\n",
    "        \n",
    "        params = params - learning_rate * grad\n",
    "        \n",
    "        print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is a training loop that gives us more info for each epoch\n",
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c,\n",
    "                  print_params=True):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w, b = params\n",
    "\n",
    "        t_p = model(t_u, w, b)  # <1>\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b)  # <2>\n",
    "\n",
    "        params = params - learning_rate * grad\n",
    "\n",
    "        if epoch in {1, 2, 3, 10, 11, 99, 100, 4000, 5000}:  # <3>\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "            if print_params:\n",
    "                print('\\n    Params:', params)\n",
    "                print('    Grad:  ', grad,'\\n')\n",
    "        if epoch in {4, 12, 101}:\n",
    "            print('...')\n",
    "\n",
    "        if not torch.isfinite(loss).all():\n",
    "            break  # <3>\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 1763.884644\n",
      "\n",
      "    Params: tensor([-44.1730,  -0.8260])\n",
      "    Grad:   tensor([4517.2969,   82.6000]) \n",
      "\n",
      "Epoch 2, Loss 5802485.500000\n",
      "\n",
      "    Params: tensor([2568.4014,   45.1637])\n",
      "    Grad:   tensor([-261257.4219,   -4598.9712]) \n",
      "\n",
      "Epoch 3, Loss 19408035840.000000\n",
      "\n",
      "    Params: tensor([-148527.7344,   -2616.3933])\n",
      "    Grad:   tensor([15109614.0000,   266155.7188]) \n",
      "\n",
      "...\n",
      "Epoch 10, Loss 90901154706620645225508955521810432.000000\n",
      "\n",
      "    Params: tensor([3.2144e+17, 5.6621e+15])\n",
      "    Grad:   tensor([-3.2700e+19, -5.7600e+17]) \n",
      "\n",
      "Epoch 11, Loss inf\n",
      "\n",
      "    Params: tensor([-1.8590e+19, -3.2746e+17])\n",
      "    Grad:   tensor([1.8912e+21, 3.3313e+19]) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-1.8590e+19, -3.2746e+17])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now lets train our model\n",
    "training_loop(n_epochs=100,\n",
    "             learning_rate=1e-2,\n",
    "             params=torch.tensor([1.0,0.0]),\n",
    "             t_u=t_u,\n",
    "             t_c=t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> OVERTRAINING: </b>  Wait, what happened? Our training process literally blew up, leading to losses becoming inf. This is a clear sign that params is receiving updates that are too large, and\n",
    "their values start oscillating back and forth as each update overshoots and the next\n",
    "overcorrects even more. The optimization process is unstable: it diverges instead of\n",
    "converging to a minimum. We want to see smaller and smaller updates to params, not\n",
    "larger\n",
    "\n",
    "How can we limit the magnitude of learning_rate * grad? Well, that looks easy. We\n",
    "could simply choose a smaller learning_rate, and indeed, the learning rate is one of\n",
    "the things we typically change when training does not go as well as we would like.8\n",
    " \n",
    " We usually change learning rates by orders of magnitude, so we might try with 1e-3 or\n",
    "1e-4, which would decrease the magnitude of the updates by orders of magnitude.\n",
    "Let’s go with 1e-4 and see how it works out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 1763.884644\n",
      "\n",
      "    Params: tensor([ 0.5483, -0.0083])\n",
      "    Grad:   tensor([4517.2969,   82.6000]) \n",
      "\n",
      "Epoch 2, Loss 323.090546\n",
      "\n",
      "    Params: tensor([ 0.3623, -0.0118])\n",
      "    Grad:   tensor([1859.5493,   35.7843]) \n",
      "\n",
      "Epoch 3, Loss 78.929634\n",
      "\n",
      "    Params: tensor([ 0.2858, -0.0135])\n",
      "    Grad:   tensor([765.4667,  16.5122]) \n",
      "\n",
      "...\n",
      "Epoch 10, Loss 29.105242\n",
      "\n",
      "    Params: tensor([ 0.2324, -0.0166])\n",
      "    Grad:   tensor([1.4803, 3.0544]) \n",
      "\n",
      "Epoch 11, Loss 29.104168\n",
      "\n",
      "    Params: tensor([ 0.2323, -0.0169])\n",
      "    Grad:   tensor([0.5781, 3.0384]) \n",
      "\n",
      "...\n",
      "Epoch 99, Loss 29.023582\n",
      "\n",
      "    Params: tensor([ 0.2327, -0.0435])\n",
      "    Grad:   tensor([-0.0533,  3.0226]) \n",
      "\n",
      "Epoch 100, Loss 29.022669\n",
      "\n",
      "    Params: tensor([ 0.2327, -0.0438])\n",
      "    Grad:   tensor([-0.0532,  3.0226]) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2327, -0.0438])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#again with lr=1e-4\n",
    "training_loop(n_epochs=100,\n",
    "             learning_rate=1e-4,\n",
    "             params=torch.tensor([1.0,0.0]),\n",
    "             t_u=t_u,\n",
    "             t_c=t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice—the behavior is now stable. But there’s another problem: the updates to parameters are very small, so the loss decreases very slowly and eventually stalls. We could\n",
    "obviate this issue by making learning_rate adaptive: that is, change according to the\n",
    "magnitude of updates. There are optimization schemes that do that, and we’ll see one\n",
    "toward the end of this chapter.\n",
    "\n",
    "\n",
    " However, there’s another potential troublemaker in the update term: the gradient\n",
    "itself. Let’s go back and look at grad at epoch 1 during optimization. \n",
    "\n",
    "## 5.4.4 Normalizing inputs\n",
    "\n",
    "We can see that the first-epoch gradient for the weight is about 50 times larger than\n",
    "the gradient for the bias. This means the weight and bias live in differently scaled\n",
    "spaces. If this is the case, a learning rate that’s large enough to meaningfully update\n",
    "one will be so large as to be unstable for the other; and a rate that’s appropriate for\n",
    "the other won’t be large enough to meaningfully change the first. That means we’re\n",
    "not going to be able to update our parameters unless we change something about our\n",
    "formulation of the problem. We could have individual learning rates for each parameter, but for models with many parameters, this would be too much to bother with\n",
    "\n",
    "\n",
    " There’s a simpler way to keep things in check: changing the inputs so that the gradients aren’t quite so different. We can make sure the range of the input doesn’t get\n",
    "too far from the range of –1.0 to 1.0, roughly speaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 80.364342\n",
      "\n",
      "    Params: tensor([1.7761, 0.1064])\n",
      "    Grad:   tensor([-77.6140, -10.6400]) \n",
      "\n",
      "Epoch 2, Loss 37.574917\n",
      "\n",
      "    Params: tensor([2.0848, 0.1303])\n",
      "    Grad:   tensor([-30.8623,  -2.3864]) \n",
      "\n",
      "Epoch 3, Loss 30.871077\n",
      "\n",
      "    Params: tensor([2.2094, 0.1217])\n",
      "    Grad:   tensor([-12.4631,   0.8587]) \n",
      "\n",
      "...\n",
      "Epoch 10, Loss 29.030487\n",
      "\n",
      "    Params: tensor([ 2.3232, -0.0710])\n",
      "    Grad:   tensor([-0.5355,  2.9295]) \n",
      "\n",
      "Epoch 11, Loss 28.941875\n",
      "\n",
      "    Params: tensor([ 2.3284, -0.1003])\n",
      "    Grad:   tensor([-0.5240,  2.9264]) \n",
      "\n",
      "...\n",
      "Epoch 99, Loss 22.214186\n",
      "\n",
      "    Params: tensor([ 2.7508, -2.4910])\n",
      "    Grad:   tensor([-0.4453,  2.5208]) \n",
      "\n",
      "Epoch 100, Loss 22.148710\n",
      "\n",
      "    Params: tensor([ 2.7553, -2.5162])\n",
      "    Grad:   tensor([-0.4446,  2.5165]) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 2.7553, -2.5162])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets try a simple fix by multiplying t_u by 0.1\n",
    "t_un = 0.1 * t_u\n",
    "\n",
    "training_loop(\n",
    "n_epochs = 100,\n",
    "learning_rate = 1e-2,\n",
    "params = torch.tensor([1.0, 0.0]),\n",
    "t_u = t_un,\n",
    "t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:facial_rec]",
   "language": "python",
   "name": "conda-env-facial_rec-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
