{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:3em;\">Ch5: The Mechanics of Learning\n",
    "</span>\n",
    "\n",
    "# 5.1 A timeless lesson in modeling\n",
    "\n",
    " We can argue that learning from data presumes the underlying model is not engineered to solve a specific problem and is instead capable of approximating a much wider family of functions\n",
    " \n",
    "  In this book, we’re interested in models that are not engineered for solving a specific narrow task, but that can be automatically adapted to specialize themselves for\n",
    "any one of many similar tasks using input and output pairs—in other words, general\n",
    "models trained on data relevant to the specific task at hand. In particular, PyTorch is\n",
    "designed to make it easy to create models for which the derivatives of the fitting error,\n",
    "with respect to the parameters, can be expressed analytically\n",
    "\n",
    " In this book, we’re interested in models that are not engineered for solving a specific narrow task, but that can be automatically adapted to specialize themselves for\n",
    "any one of many similar tasks using input and output pairs—in other words, general\n",
    "models trained on data relevant to the specific task at hand. In particular, PyTorch is\n",
    "designed to make it easy to create models for which the derivatives of the fitting error,\n",
    "with respect to the parameters, can be expressed analytically.  \n",
    "\n",
    "In order to make sure we get the key concepts right, we’ll start with a model\n",
    "that is a lot simpler than a deep neural network\n",
    "\n",
    "# 5.2 Learning is just parameter estimation\n",
    "\n",
    "Given input data and the corresponding desired outputs (ground truth), as\n",
    "well as initial values for the weights, the model is fed input data (forward pass), and a\n",
    "measure of the error is evaluated by comparing the resulting outputs to the ground\n",
    "truth. In order to optimize the parameter of the model—its weights—the change in\n",
    "the error following a unit change in weights (that is, the gradient of the error with\n",
    "respect to the parameters) is computed using the chain rule for the derivative of a\n",
    "composite function (backward pass). The value of the weights is then updated in the\n",
    "direction that leads to a decrease in the error\n",
    "\n",
    "When we start, we’ll be doing everything by hand,\n",
    "but by the end of the chapter we’ll be letting PyTorch do all the heavy lifting for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5000, 14.0000, 15.0000, 28.0000, 11.0000,  8.0000,  3.0000, -4.0000,\n",
       "         6.0000, 13.0000, 21.0000])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We will start by using temperature data \n",
    "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x122b89d50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#quick \n",
    "plt.scatter(t_c,t_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The\n",
    "two may be linearly related—that is, multiplying t_u by a factor and adding a constant,\n",
    "we may get the temperature in Celsius:\n",
    "<b>t_c = w * t_u + b</b>\n",
    "\n",
    " Let’s flesh it out again: we have a model with some unknown parameters, and we\n",
    "need to estimate those parameters so that the error between predicted outputs and\n",
    "measured values is as low as possible. We notice that we still need to exactly define a\n",
    "measure of the error. Such a measure, which we refer to as the loss function, should be\n",
    "high if the error is high and should ideally be as low as possible for a perfect match.\n",
    "Our optimization process should therefore aim at finding w and b so that the loss\n",
    "function is at a minimum.\n",
    "\n",
    "\n",
    "# 5.3 Less loss is what we want\n",
    "A loss function (or cost function) is a function that computes a single numerical value\n",
    "that the learning process will attempt to minimize. \n",
    "\n",
    "In our case,\n",
    "that would be the difference between the predicted temperatures t_p output by our\n",
    "model and the actual measurements: t_p – t\n",
    "\n",
    "<b> Mean Squared Error</b> is a common loss function for regression.\n",
    "\n",
    "MSE has a clear minimum at 0 and grows monotically as the prediction moves away from the true value in both directions. Because steepness of the growth also monotically increases away from the minimum, it is a <b>convex</b> function. Since our model is linear, the loss as a function of w and b are also convex\n",
    "\n",
    "\n",
    " Cases where the loss is a convex function of the model parameters are usually great to deal with because we can find a minimum very efficiently through specialized algorithms. However, we will instead use less powerful but more\n",
    "generally applicable methods in this chapter. We do so because for the deep neural networks we are ultimately interested in, the loss is not a convex function of the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For our two loss functions <b>|t_p – t_c| and (t_p – t_c)^2</b>, as shown in figure 5.4,\n",
    "we notice that the square of the differences behaves more nicely around the minimum: the derivative of the error-squared loss with respect to t_p is zero when t_p\n",
    "equals t_c. The absolute value, on the other hand, has an undefined derivative right\n",
    "where we’d like to converge. This is less of an issue in practice than it looks like, but\n",
    "we’ll stick to the square of differences for the time being.\n",
    "\n",
    "## 5.3.1 From problem back to PyTorch\n",
    "\n",
    " Now we need to set the learning process in motion and feed it actual data. \n",
    " We’ve already created our data tensors, so now let’s write out the model as a\n",
    "Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re expecting t_u, w, and b to be the input tensor, weight parameter, and bias\n",
    "parameter, respectively. In our model, the parameters will be PyTorch scalars (aka zero-dimensional tensors), and the product operation will use broadcasting to yield\n",
    "the returned tensors.\n",
    "\n",
    "Now lets define loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p,t_c):\n",
    "    '''\n",
    "    Mean Squared Loss\n",
    "    '''\n",
    "    squared_dffs = (t_p - t_c)**2\n",
    "    return squared_dffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,\n",
       "        48.4000, 60.4000, 68.4000])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now lets initialize w and b and create the model\n",
    "w = torch.ones(())\n",
    "b = torch.zeros(())\n",
    "\n",
    "t_p = model(t_u, w, b)\n",
    "t_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8846)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loss\n",
    "loss = loss_fn(t_p, t_c)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Broadcasting:</b>\n",
    "\n",
    "\n",
    "We mentioned broadcasting in chapter 3, and we promised to look at it more carefully\n",
    "when we need it. In our example, we have two scalars (zero-dimensional tensors) w\n",
    "and b, and we multiply them with and add them to vectors (one-dimensional tensors)\n",
    "of length b.\n",
    "Usually—and in early versions of PyTorch, too—we can only use element-wise binary\n",
    "operations such as addition, subtraction, multiplication, and division for arguments\n",
    "of the same shape. The entries in matching positions in each of the tensors will be\n",
    "used to calculate the corresponding entry in the result tensor.\n",
    "\n",
    "\n",
    "Broadcasting, which is popular in NumPy and adapted by PyTorch, relaxes this assumption for most binary operations. It uses the following rules to match tensor elements:\n",
    "- For each index dimension, counted from the back, if one of the operands is\n",
    "size 1 in that dimension, PyTorch will use the single entry along this dimension with each of the entries in the other tensor along this dimension.\n",
    "\n",
    "\n",
    "- If both sizes are greater than 1, they must be the same, and natural matching\n",
    "is used.\n",
    "\n",
    "\n",
    "- If one of the tensors has more index dimensions than the other, the entirety\n",
    "of the other tensor will be used for each entry along these dimensions.\n",
    "\n",
    "This sounds complicated (and it can be error-prone if we don’t pay close attention, which\n",
    "is why we have named the tensor dimensions as shown in section 3.4), but usually,\n",
    "we can either write down the tensor dimensions to see what happens or picture what\n",
    "happens by using space dimensions to show the broadcasting, as in the following figure.\n",
    "\n",
    "<b>Note:</b> This is for element-wise multiplication below not matrix multiplication(dot product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: x: torch.Size([]) y: torch.Size([3, 1])\n",
      "z: torch.Size([1, 3]), a: torch.Size([2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(())\n",
    "y = torch.ones(3,1)\n",
    "z = torch.ones(1,3)\n",
    "a = torch.ones(2,1,1)\n",
    "\n",
    "print(f'shapes: x: {x.shape} y: {y.shape}')\n",
    "print(f\"z: {z.shape}, a: {a.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x * y: torch.Size([3, 1])\n",
      "y * z: torch.Size([3, 3])\n",
      "y * z * a: torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"x * y:\", (x * y).shape)\n",
    "print(\"y * z:\", (y * z).shape)\n",
    "print(\"y * z * a:\", (y * z * a).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 Down along the gradient\n",
    "\n",
    "We’ll optimize the loss function with respect to the parameters using the gradient\n",
    "descent algorithm.  Gradient descent is actually a very simple idea, and it scales up surprisingly well to large neural network models with millions of parameters.\n",
    "\n",
    "## 5.4.1 Decreasing loss\n",
    "\n",
    "The idea to to compute the rate of change of the loss with respect to each parameter, and modify each parameter in the direction of decreasing loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.1\n",
    "\n",
    "#this backwards slash allows us to write on the next line, its called line continutation character\n",
    "loss_rate_of_change_w = \\\n",
    "(loss_fn(model(t_u, w + delta, b), t_c) -\n",
    "loss_fn(model(t_u, w - delta, b), t_c)) / (2.0 * delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4517.2979)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_rate_of_change_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is saying that in the neighborhood of the current values of w and b, a unit\n",
    "increase in w leads to some change in the loss. If the change is negative, then we need\n",
    "to increase w to minimize the loss, whereas if the change is positive, we need to\n",
    "decrease w\n",
    "\n",
    "It is also wise to\n",
    "change the parameters slowly in general, because the rate of change could be dramatically different at a distance from the neighborhood of the current w value. Therefore,\n",
    "we typically should scale the rate of change by a small factor. This scaling factor has\n",
    "many names; the one we use in machine learning is <b>learning_rate</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "w = w - learning_rate* loss_rate_of_change_w\n",
    "\n",
    "#Can do the same with b:\n",
    "loss_rate_of_change_b = \\\n",
    "(loss_fn(model(t_u, w, b + delta), t_c) -\n",
    "loss_fn(model(t_u, w, b - delta), t_c)) / (2.0 * delta)\n",
    "\n",
    "b=b- learning_rate * loss_rate_of_change_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This represents the basic parameter-update step for gradient descent. By reiterating\n",
    "these evaluations (and provided we choose a small enough learning rate), we will\n",
    "converge to an optimal value of the parameters for which the loss computed on the\n",
    "given data is minimal.\n",
    "\n",
    "## 5.4.2 Getting analytical\n",
    "\n",
    "In a model with two or more parameters like the one we’re\n",
    "dealing with, we compute the individual derivatives of the loss with respect to each\n",
    "parameter and put them in a vector of derivatives: the gradient.\n",
    "\n",
    "In order to compute the derivative of the loss with respect to a parameter, we can\n",
    "apply the chain rule and compute the derivative of the loss with respect to its input\n",
    "(which is the output of the model), times the derivative of the model with respect to\n",
    "the parameter:\n",
    "\n",
    "\n",
    "\n",
    "dloss_fn / d w = (dloss_fn / d t_p) * (d t_p / d w)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p,t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()\n",
    "\n",
    "#Remembering that d x^2 / d x = 2 x, we get\n",
    "def dloss_fn(t_p, t_c):\n",
    "    dsg_diffs = 2 * (t_p - t_c) / t_p.size(0)\n",
    "    return dsg_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the derivatives to the model from earlier get\n",
    "def dmodel_dw(t_u, w, b):\n",
    "    return t_u\n",
    "def dmodel_db(t_u,w,b):\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>DEFINING THE GRADIENT FUNCTION: </b>\n",
    "Putting all of this together, the function returning the gradient of the loss with respect\n",
    "to w and b is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fn(t_u, t_c, t_p, w, b):\n",
    "    dloss_dtp = dloss_fn(t_p,t_c)\n",
    "    dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b)\n",
    "    dloss_db = dloss_dtp * dmodel_db(t_u,w,b)\n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.3 Iterating to fit the model\n",
    "\n",
    "We now have everything in place to optimize our parameters. Starting from a tentative\n",
    "value for a parameter, we can iteratively apply updates to it for a fixed number of iterations, or until w and b stop changing. There are several stopping criteria; for now,\n",
    "we’ll stick to a fixed number of iterations.\n",
    "\n",
    "<b>THE TRAINING LOOP: </b>\n",
    "Since we’re at it, let’s introduce another piece of terminology. We call a training iteration during which we update the parameters for all of our training samples an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The complete training loop looks like this \n",
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c, print_params=True):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w, b = params\n",
    "        t_p = model(t_u, w, b) # forward pass\n",
    "        loss = loss_fn(t_p, t_c) \n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b) # Backward pass\n",
    "        \n",
    "        params = params - learning_rate * grad\n",
    "        \n",
    "        print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is a training loop that gives us more info for each epoch\n",
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c,\n",
    "                  print_params=True):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w, b = params\n",
    "\n",
    "        t_p = model(t_u, w, b)  # <1>\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b)  # <2>\n",
    "\n",
    "        params = params - learning_rate * grad\n",
    "\n",
    "        if epoch in {1, 2, 3, 10, 11, 99, 100, 4000, 5000}:  # <3>\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "            if print_params:\n",
    "                print('\\n    Params:', params)\n",
    "                print('    Grad:  ', grad,'\\n')\n",
    "        if epoch in {4, 12, 101}:\n",
    "            print('...')\n",
    "\n",
    "        if not torch.isfinite(loss).all():\n",
    "            break  # <3>\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 1763.884644\n",
      "\n",
      "    Params: tensor([-44.1730,  -0.8260])\n",
      "    Grad:   tensor([4517.2969,   82.6000]) \n",
      "\n",
      "Epoch 2, Loss 5802485.500000\n",
      "\n",
      "    Params: tensor([2568.4014,   45.1637])\n",
      "    Grad:   tensor([-261257.4219,   -4598.9712]) \n",
      "\n",
      "Epoch 3, Loss 19408035840.000000\n",
      "\n",
      "    Params: tensor([-148527.7344,   -2616.3933])\n",
      "    Grad:   tensor([15109614.0000,   266155.7188]) \n",
      "\n",
      "...\n",
      "Epoch 10, Loss 90901154706620645225508955521810432.000000\n",
      "\n",
      "    Params: tensor([3.2144e+17, 5.6621e+15])\n",
      "    Grad:   tensor([-3.2700e+19, -5.7600e+17]) \n",
      "\n",
      "Epoch 11, Loss inf\n",
      "\n",
      "    Params: tensor([-1.8590e+19, -3.2746e+17])\n",
      "    Grad:   tensor([1.8912e+21, 3.3313e+19]) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-1.8590e+19, -3.2746e+17])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now lets train our model\n",
    "training_loop(n_epochs=100,\n",
    "             learning_rate=1e-2,\n",
    "             params=torch.tensor([1.0,0.0]),\n",
    "             t_u=t_u,\n",
    "             t_c=t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> OVERTRAINING: </b>  Wait, what happened? Our training process literally blew up, leading to losses becoming inf. This is a clear sign that params is receiving updates that are too large, and\n",
    "their values start oscillating back and forth as each update overshoots and the next\n",
    "overcorrects even more. The optimization process is unstable: it diverges instead of\n",
    "converging to a minimum. We want to see smaller and smaller updates to params, not\n",
    "larger\n",
    "\n",
    "How can we limit the magnitude of learning_rate * grad? Well, that looks easy. We\n",
    "could simply choose a smaller learning_rate, and indeed, the learning rate is one of\n",
    "the things we typically change when training does not go as well as we would like.8\n",
    " \n",
    " We usually change learning rates by orders of magnitude, so we might try with 1e-3 or\n",
    "1e-4, which would decrease the magnitude of the updates by orders of magnitude.\n",
    "Let’s go with 1e-4 and see how it works out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 1763.884644\n",
      "\n",
      "    Params: tensor([ 0.5483, -0.0083])\n",
      "    Grad:   tensor([4517.2969,   82.6000]) \n",
      "\n",
      "Epoch 2, Loss 323.090546\n",
      "\n",
      "    Params: tensor([ 0.3623, -0.0118])\n",
      "    Grad:   tensor([1859.5493,   35.7843]) \n",
      "\n",
      "Epoch 3, Loss 78.929634\n",
      "\n",
      "    Params: tensor([ 0.2858, -0.0135])\n",
      "    Grad:   tensor([765.4667,  16.5122]) \n",
      "\n",
      "...\n",
      "Epoch 10, Loss 29.105242\n",
      "\n",
      "    Params: tensor([ 0.2324, -0.0166])\n",
      "    Grad:   tensor([1.4803, 3.0544]) \n",
      "\n",
      "Epoch 11, Loss 29.104168\n",
      "\n",
      "    Params: tensor([ 0.2323, -0.0169])\n",
      "    Grad:   tensor([0.5781, 3.0384]) \n",
      "\n",
      "...\n",
      "Epoch 99, Loss 29.023582\n",
      "\n",
      "    Params: tensor([ 0.2327, -0.0435])\n",
      "    Grad:   tensor([-0.0533,  3.0226]) \n",
      "\n",
      "Epoch 100, Loss 29.022669\n",
      "\n",
      "    Params: tensor([ 0.2327, -0.0438])\n",
      "    Grad:   tensor([-0.0532,  3.0226]) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2327, -0.0438])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#again with lr=1e-4\n",
    "training_loop(n_epochs=100,\n",
    "             learning_rate=1e-4,\n",
    "             params=torch.tensor([1.0,0.0]),\n",
    "             t_u=t_u,\n",
    "             t_c=t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice—the behavior is now stable. But there’s another problem: the updates to parameters are very small, so the loss decreases very slowly and eventually stalls. We could\n",
    "obviate this issue by making learning_rate adaptive: that is, change according to the\n",
    "magnitude of updates. There are optimization schemes that do that, and we’ll see one\n",
    "toward the end of this chapter.\n",
    "\n",
    "\n",
    " However, there’s another potential troublemaker in the update term: the gradient\n",
    "itself. Let’s go back and look at grad at epoch 1 during optimization. \n",
    "\n",
    "## 5.4.4 Normalizing inputs\n",
    "\n",
    "We can see that the first-epoch gradient for the weight is about 50 times larger than\n",
    "the gradient for the bias. This means the weight and bias live in differently scaled\n",
    "spaces. If this is the case, a learning rate that’s large enough to meaningfully update\n",
    "one will be so large as to be unstable for the other; and a rate that’s appropriate for\n",
    "the other won’t be large enough to meaningfully change the first. That means we’re\n",
    "not going to be able to update our parameters unless we change something about our\n",
    "formulation of the problem. We could have individual learning rates for each parameter, but for models with many parameters, this would be too much to bother with\n",
    "\n",
    "\n",
    " There’s a simpler way to keep things in check: changing the inputs so that the gradients aren’t quite so different. We can make sure the range of the input doesn’t get\n",
    "too far from the range of –1.0 to 1.0, roughly speaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 80.364342\n",
      "\n",
      "    Params: tensor([1.7761, 0.1064])\n",
      "    Grad:   tensor([-77.6140, -10.6400]) \n",
      "\n",
      "Epoch 2, Loss 37.574917\n",
      "\n",
      "    Params: tensor([2.0848, 0.1303])\n",
      "    Grad:   tensor([-30.8623,  -2.3864]) \n",
      "\n",
      "Epoch 3, Loss 30.871077\n",
      "\n",
      "    Params: tensor([2.2094, 0.1217])\n",
      "    Grad:   tensor([-12.4631,   0.8587]) \n",
      "\n",
      "...\n",
      "Epoch 10, Loss 29.030487\n",
      "\n",
      "    Params: tensor([ 2.3232, -0.0710])\n",
      "    Grad:   tensor([-0.5355,  2.9295]) \n",
      "\n",
      "Epoch 11, Loss 28.941875\n",
      "\n",
      "    Params: tensor([ 2.3284, -0.1003])\n",
      "    Grad:   tensor([-0.5240,  2.9264]) \n",
      "\n",
      "...\n",
      "Epoch 99, Loss 22.214186\n",
      "\n",
      "    Params: tensor([ 2.7508, -2.4910])\n",
      "    Grad:   tensor([-0.4453,  2.5208]) \n",
      "\n",
      "Epoch 100, Loss 22.148710\n",
      "\n",
      "    Params: tensor([ 2.7553, -2.5162])\n",
      "    Grad:   tensor([-0.4446,  2.5165]) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 2.7553, -2.5162])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets try a simple fix by multiplying t_u by 0.1\n",
    "t_un = 0.1 * t_u\n",
    "\n",
    "training_loop(\n",
    "n_epochs = 100,\n",
    "learning_rate = 1e-2,\n",
    "params = torch.tensor([1.0, 0.0]),\n",
    "t_u = t_un,\n",
    "t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 80.364342\n",
      "Epoch 2, Loss 37.574917\n",
      "Epoch 3, Loss 30.871077\n",
      "...\n",
      "Epoch 10, Loss 29.030487\n",
      "Epoch 11, Loss 28.941875\n",
      "...\n",
      "Epoch 99, Loss 22.214186\n",
      "Epoch 100, Loss 22.148710\n",
      "...\n",
      "Epoch 4000, Loss 2.927680\n",
      "Epoch 5000, Loss 2.927648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let’s run the loop for enough iterations to see the changes in params get small. We’ll change n_epochs to 5,000:\n",
    "params = training_loop(\n",
    "n_epochs = 5000,\n",
    "learning_rate = 1e-2,\n",
    "params = torch.tensor([1.0, 0.0]),\n",
    "t_u = t_un,\n",
    "t_c = t_c,\n",
    "print_params = False)\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a Python trick called argument unpacking here: *params means to pass the\n",
    "elements of params as individual arguments. In Python, this is usually done with lists\n",
    "or tuples, but we can also use argument unpacking with PyTorch tensors, which are\n",
    "split along the leading dimension. So here, model(t_un, *params) is equivalent to\n",
    "model(t_un, params[0], params[1])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets try plotting the data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "t_p = model(t_un, *params) #The *params means we are using argument unpacking\n",
    "\n",
    "\n",
    "fig = plt.figure(dpi=100)\n",
    "plt.xlabel(\"Temperature (°Fahrenheit)\")\n",
    "plt.ylabel(\"Temperature (°Celsius)\")\n",
    "\n",
    "plt.plot(t_u.numpy(), t_p.detach().numpy())  #plotiing the raw unknown values\n",
    "plt.plot(t_u.numpy(), t_c.numpy(), 'o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5 PyTorch’s autograd: Backpropagating all things\n",
    "We just saw a simple example of backpropagation: we computed the gradient of a composition of functions—the model and the loss—with\n",
    "respect to their innermost parameters (w and b) by propagating derivatives backward\n",
    "using the chain rule. The basic requirement here is that all functions we’re dealing\n",
    "with can be differentiated analytically. If this is the case, we can compute the gradient—what we earlier called “the rate of change of the loss”—with respect to the\n",
    "parameters in one sweep.\n",
    "\n",
    "## 5.5.1 Computing the gradient automatically\n",
    "\n",
    "This is when PyTorch tensors come to the rescue, with a PyTorch component called\n",
    "autograd. PyTorch tensors can remember where they come from, in terms of the operations and\n",
    "parent tensors that originated them, and they can automatically provide the chain of\n",
    "derivatives of such operations with respect to their inputs. This means we won’t need\n",
    "to derive our model by hand;10 given a forward expression, no matter how nested,\n",
    "PyTorch will automatically provide the gradient of that expression with respect to its\n",
    "input parameters.\n",
    "\n",
    "\n",
    "<b>APPLYING AUTOGRAD</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b\n",
    "\n",
    "\n",
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()\n",
    "\n",
    "#Initialize tensor\n",
    "params = torch.tensor([1.0,0.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USING THE GRAD ATTRIBUTE\n",
    "Notice the requires_grad=True argument to the tensor constructor? That argument\n",
    "is telling PyTorch to track the entire family tree of tensors resulting from operations\n",
    "on params. In other words, any tensor that will have params as an ancestor will have\n",
    "access to the chain of functions that were called to get from params to that tensor. In\n",
    "case these functions are differentiable (and most PyTorch tensor operations will be),\n",
    "the value of the derivative will be automatically populated as a grad attribute of the\n",
    "params tensor.\n",
    "\n",
    "\n",
    " In general, all PyTorch tensors have an attribute named grad. Normally, it’s None:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.grad is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we have to do to populate it is to start with a tensor with requires_grad set to\n",
    "True, then call the model and compute the loss, and then call backward on the loss\n",
    "tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4517.2969,   82.6000])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(model(t_u, *params), t_c)\n",
    "loss.backward()\n",
    "\n",
    "params.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the grad attribute of params contains the derivatives of the loss with\n",
    "respect to each element of params.\n",
    " \n",
    " When we compute our loss while the parameters w and b require gradients, in\n",
    "addition to performing the actual computation, PyTorch creates the autograd graph\n",
    "with the operations as nodes. When we call loss.backward(), PyTorch traverses this graph in the reverse\n",
    "direction to compute the gradients, as shown by the arrows in the bottom row of\n",
    "the figure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> ACCUMULATING GRAD FUNCTIONS</b>\n",
    "\n",
    "We could have any number of tensors with requires_grad set to True and any composition of functions. In this case, PyTorch would compute the derivatives of the loss\n",
    "throughout the chain of functions (the computation graph) and accumulate their values in the grad attribute of those tensors (the leaf nodes of the graph).\n",
    "\n",
    "\n",
    "<b> Alert! Big gotcha ahead. This is something PyTorch newcomers—and a lot of more\n",
    "experienced folks, too—trip up on regularly. We just wrote accumulate, not store.\n",
    "WARNING Calling backward will lead derivatives to accumulate at leaf nodes.\n",
    "We need to zero the gradient explicitly after using it for parameter updates. </b>\n",
    "\n",
    "Let’s repeat together: calling backward will lead derivatives to accumulate at leaf nodes.\n",
    "So if backward was called earlier, the loss is evaluated again, backward is called again\n",
    "(as in any training loop), and the gradient at each leaf is accumulated (that is,\n",
    "summed) on top of the one computed at the previous iteration, which leads to an\n",
    "incorrect value for the gradient.\n",
    "\n",
    "\n",
    " In order to prevent this from occurring, <b>we need to zero the gradient explicitly at each\n",
    "iteration</b>. We can do this easily using the in-place zero_ method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.grad is not None:\n",
    "    params.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>NOTE:</b> You might be curious why zeroing the gradient is a required step\n",
    "instead of zeroing happening automatically whenever we call backward.\n",
    "Doing it this way provides more flexibility and control when working with gradients in complicated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets put it all together\n",
    "\n",
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        if params.grad is not None:\n",
    "            params.grad.zero_()\n",
    "            \n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        with torch.no_grad():  #So we dont accumlate gradients in the backward pass\n",
    "            params -= learning_rate * params.grad\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our code updating params is not quite as straightforward as we might have\n",
    "expected. First, we are wrap the update in a\n",
    "no_grad context using the Python with statement. This means within the with block,\n",
    "the PyTorch autograd mechanism should look away:\n",
    "that is, not add edges to the forward graph. In fact, when we are executing this bit of code, the forward graph that\n",
    "PyTorch records is consumed when we call backward, leaving us with the params leaf\n",
    "node. But now we want to change this leaf node before we start building a fresh forward graph on top of it. While this use case is usually wrapped inside the optimizers\n",
    "we discuss in section 5.5.2, we will take a closer look when we see another common use\n",
    "of no_grad in section 5.5.4.\n",
    "\n",
    "\n",
    "\n",
    " Second, we update params in place. This means we keep the same params tensor\n",
    "around but subtract our update from it. When using autograd, we usually avoid inplace updates because PyTorch’s autograd engine might need the values we would be\n",
    "modifying for the backward pass. Here, however, we are operating without autograd,\n",
    "and it is beneficial to keep the params tensor. Not replacing the parameters by assigning new tensors to their variable name will become crucial when we register our\n",
    "parameters with the optimizer in section 5.5.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860116\n",
      "Epoch 1000, Loss 3.828538\n",
      "Epoch 1500, Loss 3.092191\n",
      "Epoch 2000, Loss 2.957697\n",
      "Epoch 2500, Loss 2.933134\n",
      "Epoch 3000, Loss 2.928648\n",
      "Epoch 3500, Loss 2.927830\n",
      "Epoch 4000, Loss 2.927679\n",
      "Epoch 4500, Loss 2.927652\n",
      "Epoch 5000, Loss 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets try the full thing\n",
    "training_loop(\n",
    "n_epochs = 5000,\n",
    "learning_rate = 1e-2,\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True),  #adding requires_grad=True is key \n",
    "t_u = t_un,\n",
    "t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5.2 Optimizers a la carte\n",
    "\n",
    " The torch module has an optim submodule where we can find classes implementing different\n",
    "optimization algorithms. This saves us from the boilerplate busywork\n",
    "of having to update each and every parameter to our model ourselves. Here’s an abridged list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'Optimizer',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'lr_scheduler']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "dir(optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every optimizer constructor takes a list of parameters (aka PyTorch tensors, typically\n",
    "with requires_grad set to True) as the first input. All parameters passed to the optimizer are retained inside the optimizer object so the optimizer can update their values and access their grad attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each optimizer exposes two methods: zero_grad and step. \n",
    "- <b>zero_grad:</b> zeroes the\n",
    "grad attribute of all the parameters passed to the optimizer upon construction. \n",
    "- <b>step:</b>\n",
    "updates the value of those parameters according to the optimization strategy implemented by the specific optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create params and an optimizer\n",
    "params = torch.tensor([1.0,0.0], requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.SGD([params], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here SGD stands for stochastic gradient descent. Actually, the optimizer itself is exactly a\n",
    "vanilla gradient descent (as long as the momentum argument is set to 0.0, which is the\n",
    "default). The term stochastic comes from the fact that the gradient is typically obtained\n",
    "by averaging over a random subset of all input samples, called a minibatch. However, the\n",
    "optimizer does not know if the loss was evaluated on all the samples (vanilla) or a random subset of them (stochastic), so the algorithm is literally the same in the two cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.5483e-01, -8.2600e-04], requires_grad=True)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets try it out\n",
    "t_p = model(t_u, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of params is updated upon calling step without us having to touch it ourselves! What happens is that the optimizer looks into params.grad and updates\n",
    "params, subtracting learning_rate times grad from it, exactly as in our former handrolled code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7761, 0.1064], requires_grad=True)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One thing we need to add is zero the gradients\n",
    "\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "t_p = model(t_un, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets put it all together\n",
    "def training_loop(n_epochs, optimizer, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 500 == 0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860116\n",
      "Epoch 1000, Loss 3.828538\n",
      "Epoch 1500, Loss 3.092191\n",
      "Epoch 2000, Loss 2.957697\n",
      "Epoch 2500, Loss 2.933134\n",
      "Epoch 3000, Loss 2.928648\n",
      "Epoch 3500, Loss 2.927830\n",
      "Epoch 4000, Loss 2.927679\n",
      "Epoch 4500, Loss 2.927652\n",
      "Epoch 5000, Loss 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)  #important we use the same params in model and optimizer\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 5000,\n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    t_u = t_un,\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>TESTING OTHER OPTIMIZERS</b>\n",
    "\n",
    "In order to test more optimizers, all we have to do is instantiate a different optimizer,\n",
    "say Adam, instead of SGD. The rest of the code stays as it is. \n",
    "\n",
    " We won’t go into much detail about Adam; suffice to say that it is a more sophisticated optimizer in which the learning rate is set adaptively. In addition, it is a lot less\n",
    "sensitive to the scaling of the parameters—so insensitive that we can go back to using the original (non-normalized) input t_u, and even increase the learning rate to 1e-1,\n",
    "and Adam won’t even blink:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.612903\n",
      "Epoch 1000, Loss 3.086700\n",
      "Epoch 1500, Loss 2.928578\n",
      "Epoch 2000, Loss 2.927646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  0.5367, -17.3021], requires_grad=True)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-1\n",
    "optimizer = optim.Adam([params], lr=learning_rate)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 2000,\n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    t_u = t_u,\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer is not the only flexible part of our training loop. Let’s turn our attention to the model. In order to train a neural network on the same data and the same\n",
    "loss, all we would need to change is the model function. It wouldn’t make particular\n",
    "sense in this case, since we know that converting Celsius to Fahrenheit amounts to a\n",
    "linear transformation, but we’ll do it anyway in chapter 6. We’ll see quite soon that\n",
    "neural networks allow us to remove our arbitrary assumptions about the shape of the\n",
    "function we should be approximating. Even so, we’ll see how neural networks manage\n",
    "to be trained even when the underlying processes are highly nonlinear (such in the\n",
    "case of describing an image with a sentence, as we saw in chapter 2).\n",
    "\n",
    "\n",
    "## 5.5.3 Training, validation, and overfitting\n",
    "\n",
    " The first action we can take to combat overfitting is recognizing that it might happen. In order to do so we must take a few data points\n",
    "out of our dataset (the validation set) and only fit our model on the remaining data\n",
    "points (the training set),\n",
    "\n",
    "<b>rule 1:</b> if the training loss is not decreasing, chances are the\n",
    "model is too simple for the data. The other possibility is that our data just doesn’t contain meaningful information that lets it explain the output\n",
    "\n",
    "<b>rule 2:</b>  if the training loss and the validation loss\n",
    "diverge, we’re overfitting\n",
    "\n",
    "\n",
    "One solution is adding penalization terms to the loss function, to\n",
    "make it cheaper for the model to behave more smoothly and change more slowly (up\n",
    "to a point). Another is to add noise to the input samples, to artificially create new data\n",
    "points in between training data samples and force the model to try to fit those, too.\n",
    "There are several other ways, all of them somewhat related to these. But the best favor\n",
    "we can do to ourselves, at least as a first move, is to make our model simpler. From an\n",
    "intuitive standpoint, a simpler model may not fit the training data as perfectly as a\n",
    "more complicated model would, but it will likely behave more regularly in between\n",
    "data points.\n",
    "\n",
    " Therefore, in order to choose the right size for a neural network model in\n",
    "terms of parameters, the process is based on two steps: increase the size until it fits,\n",
    "and then scale it down until it stops overfitting.\n",
    " We’ll see more about this in chapter 12—we’ll discover that our life will be a balancing act between fitting and overfitting\n",
    " \n",
    " \n",
    "<b> Splitting a Dataset</b>\n",
    "\n",
    "Shuffling the elements of a tensor amounts to finding a permutation of its indices.\n",
    "The randperm function does exactly this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([10,  8,  6,  7,  2,  1,  3,  0,  4]), tensor([5, 9]))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples= t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "train_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Got the indices now lets create the train and val sets\n",
    "train_t_u = t_u[train_indices]\n",
    "train_t_c = t_c[train_indices]\n",
    "\n",
    "val_t_u = t_u[val_indices]\n",
    "val_t_c = t_c[val_indices]\n",
    "\n",
    "train_t_un = 0.1 * train_t_u\n",
    "val_t_un = 0.1 * val_t_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training loop doesn’t really change. We just want to additionally evaluate the validation loss at every epoch, to have a chance to recognize whether we’re overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u,\n",
    "                  train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "        \n",
    "        val_t_p = model(val_t_u, *params)\n",
    "        val_loss = loss_fn(val_t_p, val_t_c)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()  #Notice that there is no val_loss.backward() since we dnt want to train on it\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch <=3 or epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\" f\" Validation loss {val_loss.item():.4f}\")\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 91.7660, Validation loss 29.0568\n",
      "Epoch 2, Training loss 43.7766, Validation loss 2.3025\n",
      "Epoch 3, Training loss 36.0900, Validation loss 3.5195\n",
      "Epoch 500, Training loss 7.0920, Validation loss 4.6118\n",
      "Epoch 1000, Training loss 3.4116, Validation loss 4.0901\n",
      "Epoch 1500, Training loss 2.9273, Validation loss 3.9970\n",
      "Epoch 2000, Training loss 2.8636, Validation loss 3.9759\n",
      "Epoch 2500, Training loss 2.8552, Validation loss 3.9700\n",
      "Epoch 3000, Training loss 2.8541, Validation loss 3.9680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.4240, -17.2490], requires_grad=True)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 3000,\n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    train_t_u = train_t_un,\n",
    "    val_t_u = val_t_un,\n",
    "    train_t_c = train_t_c,\n",
    "    val_t_c = val_t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5.4 Autograd nits and switching it off\n",
    "\n",
    " The curious reader will have an embryo of a question at this point. The model is\n",
    "evaluated twice—once on train_t_u and once on val_t_u—and then backward is\n",
    "called. Won’t this confuse autograd? Won’t backward be influenced by the values generated during the pass on the validation set?\n",
    "\n",
    " Luckily for us, this isn’t the case. The first line in the training loop evaluates model\n",
    "on train_t_u to produce train_t_p. Then train_loss is evaluated from train_t_p.\n",
    "This creates a computation graph that links train_t_u to train_t_p to train_loss.\n",
    "When model is evaluated again on val_t_u, it produces val_t_p and val_loss. In this\n",
    "case, a separate computation graph will be created that links val_t_u to val_t_p to\n",
    "val_loss. Separate tensors have been run through the same functions, model and\n",
    "loss_fn, generating separate computation graphs\n",
    "\n",
    "\n",
    "The only tensors these two graphs have in common are the parameters. When we call\n",
    "backward on train_loss, we run backward on the first graph. In other words, we\n",
    "accumulate the derivatives of train_loss with respect to the parameters based on the\n",
    "computation generated from train_t_u.\n",
    " If we (incorrectly) called backward on val_loss as well, we would accumulate the\n",
    "derivatives of val_loss with respect to the parameters on the same leaf nodes.\n",
    "\n",
    " Calling backward on val_loss would lead to gradients accumulating in the params tensor, on top of those generated during the train_loss.backward() call. In this case, we would effectively train our model on the whole dataset (both\n",
    "training and validation), since the gradient would depend on both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> There’s another element for discussion here. </b>\n",
    " \n",
    " Since we’re not ever calling backward on val_loss, why are we building the graph in the first place? We could in fact\n",
    "just call model and loss_fn as plain functions, without tracking the computation.\n",
    "However optimized, building the autograd graph comes with additional costs that we\n",
    "could totally forgo during the validation pass, especially when the model has millions\n",
    "of parameters.\n",
    "\n",
    " In order to address this, PyTorch allows us to switch off autograd when we don’t\n",
    "need it, using the torch.no_grad context manager. We can make sure this works by\n",
    "checking the value of the requires_grad attribute on the val_loss tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_t_p = model(val_t_u, *params)\n",
    "            val_loss = loss_fn(val_t_p, val_t_c)\n",
    "            assert val_loss.requires_grad == False  #Dont need this but makes sure grad is off\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the related set_grad_enabled context, we can also condition the code to run\n",
    "with autograd enabled or disabled, according to a Boolean expression—typically indicating whether we are running in training or inference mode. We could, for instance,\n",
    "define a calc_forward function that takes data as input and runs model and loss_fn\n",
    "with or without autograd according to a Boolean train_is argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_forward(t_u, t_c, is_train=True):\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Redefine the model to be w2 * t_u ** 2 + w1 * t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w1, w2, b):\n",
    "    return (w2 * t_u)**2 + w1*t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 681.0540, Validation loss 652.1264\n",
      "Epoch 2, Training loss 46.4894, Validation loss 54.6286\n",
      "Epoch 3, Training loss 27.5840, Validation loss 30.2537\n",
      "Epoch 500, Training loss 12.3546, Validation loss 4.5737\n",
      "Epoch 1000, Training loss 9.7718, Validation loss 3.3101\n",
      "Epoch 1500, Training loss 8.0027, Validation loss 2.4447\n",
      "Epoch 2000, Training loss 6.7918, Validation loss 1.8536\n",
      "Epoch 2500, Training loss 5.9630, Validation loss 1.4505\n",
      "Epoch 3000, Training loss 5.3955, Validation loss 1.1761\n",
      "Epoch 3500, Training loss 5.0064, Validation loss 0.9895\n",
      "Epoch 4000, Training loss 4.7391, Validation loss 0.8627\n",
      "Epoch 4500, Training loss 4.5550, Validation loss 0.7767\n",
      "Epoch 5000, Training loss 4.4275, Validation loss 0.7185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.9373,  0.7554, -0.9603], requires_grad=True)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0,1.0,0.0], requires_grad=True)\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 5000,\n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    train_t_u = train_t_un,\n",
    "    val_t_u = val_t_un,\n",
    "    train_t_c = train_t_c,\n",
    "    val_t_c = val_t_c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:facial_rec]",
   "language": "python",
   "name": "conda-env-facial_rec-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
