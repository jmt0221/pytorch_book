{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:3em;\">Ch6: Using a neural network to fit the data </span>\n",
    "\n",
    "In this chapter, we will make some changes to our model architecture: we’re going to\n",
    "implement a full artificial neural network to solve our temperature-conversion\n",
    "problem. We’ll continue using our training loop from the last chapter, along with our\n",
    "Fahrenheit-to-Celsius samples split into training and validation sets. We could start to\n",
    "use a quadratic model: rewriting model as a quadratic function of its input \n",
    "\n",
    "# 6.1 Artificial neurons\n",
    "\n",
    "The basic building block of these complicated functions is the neuron, as illustrated in\n",
    "figure 6.2. At its core, it is nothing but a linear transformation of the input (for example, multiplying the input by a number [the weight] and adding a constant [the bias])\n",
    "followed by the application of a fixed nonlinear function (referred to as the activation\n",
    "function).\n",
    "\n",
    " Mathematically, we can write this out as o = f(w * x + b), with x as our input, w our\n",
    "weight or scaling factor, and b as our bias or offset. f is our activation function.\n",
    "\n",
    "\n",
    "## 6.1.1 Composing a multilayer network\n",
    "A multilayer neural network is made up of a composition of functions:\n",
    "\n",
    "x_1 = f(w_0 * x + b_0)\n",
    "\n",
    "x_2 = f(w_1 * x_1 + b_1)\n",
    "\n",
    "...\n",
    "\n",
    "y = f(w_n * x_n + b_n)\n",
    "\n",
    "where the output of a layer of neurons is used as an input for the following layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.2 Understanding the error function\n",
    "\n",
    " Our linear model and\n",
    "error-squared loss function had a convex error curve with a singular, clearly defined\n",
    "minimum. If we were to use other methods, we could solve for the parameters minimizing the error function automatically and definitively. That means that our parameter updates were attempting to estimate that singular correct answer as best they could.\n",
    "\n",
    " Neural networks do not have that same property of a convex error surface, even\n",
    "when using the same error-squared loss function! There’s no single right answer for\n",
    "each parameter we’re attempting to approximate. Instead, we are trying to get all of\n",
    "the parameters, when acting in concert, to produce a useful output. Since that useful\n",
    "output is only going to approximate the truth, there will be some level of imperfection.\n",
    "\n",
    " A big part of the reason neural networks have non-convex error surfaces is due to\n",
    "the activation function. The ability of an ensemble of neurons to approximate a very\n",
    "wide range of useful functions depends on the combination of the linear and nonlinear behavior inherent to each neuron. \n",
    "\n",
    "\n",
    "## 6.1.3 All we need is activation\n",
    "\n",
    " The activation\n",
    "function plays two important roles:\n",
    "-  In the inner parts of the model, it allows the output function to have different\n",
    "slopes at different values—something a linear function by definition cannot do.\n",
    "By trickily composing these differently sloped parts for many outputs, neural\n",
    "networks can approximate arbitrary functions\n",
    "\n",
    "\n",
    "- At the last layer of the network, it has the role of concentrating the outputs of\n",
    "the preceding linear operation into a given range.\n",
    "\n",
    "## 6.1.4 More activation functions\n",
    "- Tanh\n",
    "- Sigmoid\n",
    "- ReLU\n",
    "- PreLU\n",
    "- Leaky ReLU\n",
    "- Softplus\n",
    "- HardTanh\n",
    "- And many more\n",
    "\n",
    "## 6.1.5 Choosing the best activation function\n",
    "\n",
    "Activation functions are curious, because with such a wide variety of proven successful\n",
    "ones, it’s clear that there are few, if any, strict\n",
    "requirements. As such, we’re going to discuss some generalities about activation functions that can probably be trivially disproved in the specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>By definition activation functions  are:</b>\n",
    "- Are nonlinear. Repeated applications of (w*x+b) without an activation function\n",
    "results in a function of the same (affine linear) form. The nonlinearity allows\n",
    "the overall network to approximate more complex functions.\n",
    "\n",
    "\n",
    "- Are differentiable, so that gradients can be computed through them. Point discontinuities, as we can see in Hardtanh or ReLU, are fine.\n",
    "\n",
    "Without these characteristics, the network either falls back to being a linear model or\n",
    "becomes difficult to train.\n",
    "\n",
    "<b>The following are True for the functions:</b>\n",
    "- They have at least one sensitive range, where nontrivial changes to the input\n",
    "result in a corresponding nontrivial change to the output. This is needed for\n",
    "training.\n",
    "\n",
    "\n",
    "- Many of them have an insensitive (or saturated) range, where changes to the\n",
    "input result in little or no change to the output\n",
    "\n",
    "\n",
    "<b> Often (but far from universally so), the activation function will have at least one of\n",
    "these:</b>\n",
    "- A lower bound that is approached (or met) as the input goes to negative infinity\n",
    "\n",
    "\n",
    "- A similar-but-inverse upper bound for positive infinity\n",
    "\n",
    "\n",
    "Thinking of what we know about how backpropagation works, we can figure out that\n",
    "the errors will propagate backward through the activation more effectively when the\n",
    "inputs are in the response range, while errors will not greatly affect neurons for which the input is saturated (since the gradient will be close to zero, due to the flat area\n",
    "around the output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put together, all this results in a pretty powerful mechanism: we’re saying that in a\n",
    "network built out of linear + activation units, when different inputs are presented to\n",
    "the network,\n",
    "\n",
    "1.  different units will respond in different ranges for the same inputs\n",
    "2. the errors associated with those inputs will primarily affect the neurons operating in the sensitive range, leaving other units more or less unaffected by the learning process\n",
    "\n",
    "In addition, thanks to the fact that derivatives of the activation with\n",
    "respect to its inputs are often close to 1 in the sensitive range, estimating the parameters of the linear transformation through gradient descent for the units that operate\n",
    "in that range will look a lot like the linear fit we have seen previously.\n",
    "\n",
    " Different combinations of units will respond to inputs in different ranges, and those parameters for\n",
    "those units are relatively easy to optimize through gradient descent, since learning will\n",
    "behave a lot like that of a linear function until the output saturates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.6 What learning means for a neural network\n",
    " What makes using\n",
    "deep neural networks so attractive is that it saves us from worrying too much about the\n",
    "exact function that represents our data—whether it is quadratic, piecewise polynomial, or something else. With a deep neural network model, we have a universal\n",
    "approximator and a method to estimate its parameters. This approximator can be customized to our needs, in terms of model capacity and its ability to model complicated\n",
    "input/output relationships, just by composing simple building blocks. \n",
    "\n",
    "# 6.2 The PyTorch nn module\n",
    "\n",
    " PyTorch has a whole submodule dedicated to neural networks, called torch.nn. It\n",
    "contains the building blocks needed to create all sorts of neural network architectures. Those building blocks are called modules in PyTorch parlance (such building\n",
    "blocks are often referred to as layers in other frameworks). A PyTorch module is a\n",
    "Python class deriving from the nn.Module base class. A module can have one or more\n",
    "Parameter instances as attributes, which are tensors whose values are optimized\n",
    "during the training process (think w and b in our linear model). A module can also\n",
    "have one or more submodules (subclasses of nn.Module) as attributes, and it will be\n",
    "able to track their parameters as well.\n",
    "\n",
    "<b>NOTE:</b> The submodules must be top-level attributes, not buried inside list or\n",
    "dict instances! Otherwise, the optimizer will not be able to locate the submodules (and, hence, their parameters). For situations where your model\n",
    "requires a list or dict of submodules, PyTorch provides nn.ModuleList and\n",
    "nn.ModuleDict.\n",
    "\n",
    "\n",
    "Unsurprisingly, we can find a subclass of nn.Module called nn.Linear, which applies\n",
    "an affine transformation to its input (via the parameter attributes weight and bias)\n",
    "and is equivalent to what we implemented earlier in our thermometer experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.1 Using __call__ rather than forward\n",
    "\n",
    "All PyTorch-provided subclasses of nn.Module have their<b> __ call __ </b>method defined.\n",
    "This allows us to instantiate an nn.Linear and call it as if it was a function, like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c).unsqueeze(1) # <1>\n",
    "t_u = torch.tensor(t_u).unsqueeze(1) # <1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5, 1, 3, 8, 6, 2, 7, 4, 0]), tensor([ 9, 10]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "train_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_u_train = t_u[train_indices]\n",
    "t_c_train = t_c[train_indices]\n",
    "\n",
    "t_u_val = t_u[val_indices]\n",
    "t_c_val = t_c[val_indices]\n",
    "\n",
    "t_un_train = 0.1 * t_u_train\n",
    "t_un_val = 0.1 * t_u_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.8320],\n",
       "        [-3.1391]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model = nn.Linear(1,1)\n",
    "linear_model(t_un_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling an instance of nn.Module with a set of arguments ends up calling a method\n",
    "named forward with the same arguments. The forward method is what executes the\n",
    "forward computation, while __call__ does other rather important chores before and\n",
    "after calling forward. So, it is technically possible to call forward directly, and it will\n",
    "produce the same output as __call__, but this should not be done from user code:\n",
    "\n",
    "y = model(x) --> Correct\n",
    "\n",
    "y = model.forward(x) --> Wrong\n",
    "\n",
    "## 6.2.2 Returning to the linear model\n",
    "\n",
    " The constructor to nn.Linear accepts three arguments: the\n",
    "number of input features, the number of output features, and whether the linear\n",
    "model includes a bias or not (defaults to true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.8320],\n",
       "        [-3.1391]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model(t_un_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of features in our case just refers to the size of the input and the output\n",
    "tensor for the module, so 1 and 1. We have an instance of nn.Linear with one input and one output feature. That\n",
    "only requires one weight and one bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3838]], requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.5137], requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8975], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Can call the model as such\n",
    "x = torch.ones(1)\n",
    "linear_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Modules expect the zeroth dimension of the input to be the number of samples in the batch.\n",
    " Thus, assuming we need to run nn.Linear on 10 samples, we can create an\n",
    "input tensor of size B × Nin, where B is the size of the batch and Nin is the number of\n",
    "input features, and run it once through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8975],\n",
       "        [-0.8975],\n",
       "        [-0.8975],\n",
       "        [-0.8975],\n",
       "        [-0.8975],\n",
       "        [-0.8975],\n",
       "        [-0.8975],\n",
       "        [-0.8975],\n",
       "        [-0.8975],\n",
       "        [-0.8975]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(10,1)\n",
    "linear_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason we want to do this batching is multifaceted. \n",
    "- One big motivation is to make\n",
    "sure the computation we’re asking for is big enough to saturate the computing\n",
    "resources we’re using to perform the computation. \n",
    "\n",
    "- GPUs in particular are highly parallelized, so a single input on a small model will leave most of the computing units idle.\n",
    "By providing batches of inputs, the calculation can be spread across the otherwise-idle\n",
    "units, which means the batched results come back just as quickly as a single result\n",
    "would. \n",
    "\n",
    "- Another benefit is that some advanced models use statistical information from\n",
    "the entire batch, and those statistics get better with larger batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([11, 1]), torch.Size([11, 1]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can see that the unsqueeze added a batch size dimension\n",
    "t_c.shape,t_u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets update training code\n",
    "linear_model = nn.Linear(1,1)\n",
    "optimizer = optim.SGD(linear_model.parameters(), #adds our models parameters to the optimizer\n",
    "                     lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.2679]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.1805], requires_grad=True)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we can look use parameters method to look at all parameters in a model\n",
    "list(linear_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This call recurses into submodules defined in the module’s init constructor and\n",
    "returns a flat list of all parameters encountered, so that we can conveniently pass it to\n",
    "the optimizer constructor as we did previously. This allows us to easily step through the model and update parameters accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val, t_c_train, t_c_val):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        t_p_train = model(t_u_train)\n",
    "        loss_train = loss_fn(t_p_train, t_c_train)\n",
    "        \n",
    "        t_p_val = model(t_u_val)\n",
    "        \n",
    "        loss_val = loss_fn(t_p_val, t_c_val)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch == 1 or epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {loss_train.item():.4f},\"\n",
    "            f\" Validation loss {loss_val.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It hasn’t changed practically at all, except that now we don’t pass params explicitly to\n",
    "model since the model itself holds its Parameters internally.\n",
    " There’s one last bit that we can leverage from torch.nn: the loss. Indeed, nn comes\n",
    "with several common loss functions, among them nn.MSELoss.  Loss functions\n",
    "in nn are still subclasses of nn.Module, so we will create an instance and call it as a\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 100.4855, Validation loss 187.2295\n",
      "Epoch 1000, Training loss 3.3979, Validation loss 4.2125\n",
      "Epoch 2000, Training loss 2.8067, Validation loss 3.6172\n",
      "Epoch 3000, Training loss 2.7946, Validation loss 3.5611\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[5.3307]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-17.0600], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "linear_model = nn.Linear(1, 1)\n",
    "optimizer = optim.SGD(linear_model.parameters(), lr=1e-2)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 3000,\n",
    "    optimizer = optimizer,\n",
    "    model = linear_model,\n",
    "    loss_fn = nn.MSELoss(),\n",
    "    t_u_train = t_un_train,\n",
    "    t_u_val = t_un_val,\n",
    "    t_c_train = t_c_train,\n",
    "    t_c_val = t_c_val)\n",
    "\n",
    "print()\n",
    "print(linear_model.weight)\n",
    "print(linear_model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 Finally a neural network\n",
    " There’s one last step left to take: replacing our linear model with a neural network\n",
    "as our approximating function. We said earlier that using a neural network will not\n",
    "result in a higher-quality model, since the process underlying our calibration problem\n",
    "was fundamentally linear\n",
    "\n",
    "## 6.3.1 Replacing the linear model\n",
    "We are going to keep everything else fixed, including the loss function, and only redefine model. Let’s build the simplest possible neural network: a linear module, followed\n",
    "by an activation function, feeding into another linear module.\n",
    "\n",
    "__nn provides a simple way to concatenate modules through the nn.Sequential\n",
    "container__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=13, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=13, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model = nn.Sequential(\n",
    "            nn.Linear(1,13),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(13,1))\n",
    "\n",
    "seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.2 Inspecting the parameters\n",
    "Calling model.parameters() will collect weight and bias from both the first and second linear modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([13, 1]), torch.Size([13]), torch.Size([1, 13]), torch.Size([1])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[param.shape for param in seq_model.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the tensors that the optimizer will get. Again, after we call model.backward(),\n",
    "all parameters are populated with their grad, and the optimizer then updates their values accordingly during the optimizer.step() call.  A few notes on parameters of nn.Modules. When inspecting parameters of a model\n",
    "made up of several submodules, it is handy to be able to identify parameters by name.\n",
    "There’s a method for that, called named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight torch.Size([13, 1])\n",
      "0.bias torch.Size([13])\n",
      "2.weight torch.Size([1, 13])\n",
      "2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in seq_model.named_parameters():\n",
    "    print(name,param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name of each module in Sequential is just the ordinal with which the module\n",
    "appears in the arguments. Interestingly, Sequential also accepts an OrderedDict, in which we can name each module passed to Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "seq_model = nn.Sequential(OrderedDict([\n",
    "        ('hidden_linear', nn.Linear(1, 8)),\n",
    "        ('hidden_activation', nn.Tanh()),\n",
    "        ('output_linear', nn.Linear(8, 1))\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_linear.weight torch.Size([8, 1])\n",
      "hidden_linear.bias torch.Size([8])\n",
      "output_linear.weight torch.Size([1, 8])\n",
      "output_linear.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "#allows us to get more explanatory names\n",
    "for name, param in seq_model.named_parameters():\n",
    "    print(name,param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is more descriptive; but it does not give us more flexibility in the flow of data\n",
    "through the network, which remains a purely sequential pass-through—the\n",
    "nn.Sequential is very aptly named. We will see how to take full control of the processing of input data by subclassing nn.Module ourselves in chapter 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0868], requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also access a particular Parameter by using submodules as attributes:\n",
    "seq_model.output_linear.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful for inspecting parameters or their gradients: for instance, to monitor\n",
    "gradients during training, as we did at the beginning of this chapter. Say we want to\n",
    "print out the gradients of weight of the linear portion of the hidden layer. We can run\n",
    "the training loop for the new neural network model and then look at the resulting\n",
    "gradients after the last epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 158.9396, Validation loss 300.8373\n",
      "Epoch 1000, Training loss 5.6530, Validation loss 4.9669\n",
      "Epoch 2000, Training loss 3.0531, Validation loss 8.1890\n",
      "Epoch 3000, Training loss 1.7012, Validation loss 3.9538\n",
      "Epoch 4000, Training loss 1.5439, Validation loss 4.3303\n",
      "Epoch 5000, Training loss 1.5118, Validation loss 4.5934\n",
      "output tensor([[15.9374],\n",
      "        [21.7475]], grad_fn=<AddmmBackward>)\n",
      "answer tensor([[13.],\n",
      "        [21.]])\n",
      "hidden tensor([[-0.0095],\n",
      "        [ 0.0093],\n",
      "        [-0.0046],\n",
      "        [ 0.0127],\n",
      "        [ 0.0068],\n",
      "        [ 0.0518],\n",
      "        [-0.0073],\n",
      "        [-0.0023]])\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(seq_model.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 5000,\n",
    "    optimizer = optimizer,\n",
    "    model = seq_model,\n",
    "    loss_fn = nn.MSELoss(),\n",
    "    t_u_train = t_un_train,\n",
    "    t_u_val = t_un_val,\n",
    "    t_c_train = t_c_train,\n",
    "    t_c_val = t_c_val)\n",
    "\n",
    "print('output', seq_model(t_un_val))\n",
    "print('answer', t_c_val)\n",
    "print('hidden', seq_model.hidden_linear.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.3 Comparing to the linear model\n",
    "We can also evaluate the model on all of the data and see how it differs from a line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x110e2e590>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t_range = torch.arange(20., 90.).unsqueeze(1)\n",
    "fig = plt.figure(dpi=100)\n",
    "\n",
    "plt.xlabel(\"Fahrenheit\")\n",
    "plt.ylabel(\"Celsius\")\n",
    "plt.plot(t_u.numpy(), t_c.numpy(), 'o')\n",
    "plt.plot(t_range.numpy(), seq_model(0.1 * t_range).detach().numpy(), 'c-')\n",
    "plt.plot(t_u.numpy(), seq_model(0.1*t_u).detach().numpy(),'kx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph shows that our model is overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:facial_rec]",
   "language": "python",
   "name": "conda-env-facial_rec-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
