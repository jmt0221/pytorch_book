{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:3em;\">Ch7: Telling birds from airplanes: Learning from images </span>\n",
    "\n",
    "In this chapter, we’ll keep moving ahead with building our neural network foundations. This time, we’ll turn our attention to images. Image recognition is arguably the task that made the world realize the potential of deep learning.\n",
    "\n",
    "\n",
    "# 7.1 A dataset of tiny images\n",
    "\n",
    "We will use a dataset called CIFAR-10, and, like its sibling CIFAR-100, it has been a computer vision classic for a decade. CIFAR-10 consists of 60,000 tiny 32 × 32 color (RGB) images, labeled with an integer corresponding to 1 of 10 classes: airplane (0), automobile (1), bird (2), cat (3),\n",
    "deer (4), dog (5), frog (6), horse (7), ship (8), and truck (9).\n",
    "\n",
    "Nowadays, CIFAR-10 is\n",
    "considered too simple for developing or validating new research, but it serves our\n",
    "learning purposes just fine. We will use the torchvision module to automatically\n",
    "download the dataset and load it as a collection of PyTorch tensors.\n",
    "\n",
    "## 7.1.1 Downloading CIFAR-10\n",
    "\n",
    "The first argument we provide to the CIFAR10 function is the location from which the\n",
    "data will be downloaded; the second specifies whether we’re interested in the training\n",
    "set or the validation set; and the third says whether we allow PyTorch to download the\n",
    "data if it is not found in the location specified in the first argument.\n",
    " Just like CIFAR10, the datasets submodule gives us precanned access to the most\n",
    "popular computer vision datasets, such as MNIST, Fashion-MNIST, CIFAR-100,\n",
    "SVHN, Coco, and Omniglot. In each case, the dataset is returned as a subclass of\n",
    "torch.utils.data.Dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data-unversioned/p1ch7/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data-unversioned/p1ch7/' #this is where we will download the data too\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True) #instantiates a training dataset \n",
    "cifar10_val = datasets.CIFAR10(data_path,train=False, download=True) #train=False gives us the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torchvision.datasets.cifar.CIFAR10,\n",
       " torchvision.datasets.vision.VisionDataset,\n",
       " torch.utils.data.dataset.Dataset,\n",
       " object)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can see that the Method resolution order of our cifar10 instanec includes it as a base class\n",
    "type(cifar10).__mro__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.2 The Dataset Class\n",
    "\n",
    "It’s a good time to discover what being a subclass of torch.utils.data.Dataset\n",
    "means in practice\n",
    "\n",
    " It\n",
    "is an object that is required to implement two methods: __len__ and __getitem__.\n",
    "The former should return the number of items in the dataset; the latter should return\n",
    "the item, consisting of a sample and its corresponding label (an integer index).2\n",
    " In practice, when a Python object is equipped with the __len__ method, we can\n",
    "pass it as an argument to the len Python built-in function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cifar10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, since the dataset is equipped with the __getitem__ method, we can use the\n",
    "standard subscript for indexing tuples and lists to access individual items. Here, we get\n",
    "a PIL (Python Imaging Library, the PIL package) image with our desired output—an\n",
    "integer with the value 1, corresponding to “automobile”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=32x32 at 0x11C94E910>, 1)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = cifar10[99]\n",
    "img,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We can plot these images\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.3 Dataset transforms\n",
    "\n",
    "That’s all very nice, but we’ll likely need a way to convert the PIL image to a PyTorch\n",
    "tensor before we can do anything with it. That’s where torchvision.transforms\n",
    "comes in. This module defines a set of composable, function-like objects that can be\n",
    "passed as an argument to a torchvision dataset such as datasets.CIFAR10(…), and\n",
    "that perform transformations on the data after it is loaded but before it is returned by\n",
    "  __ getitem __ \n",
    "  \n",
    "  \n",
    "Here are some of the transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CenterCrop',\n",
       " 'ColorJitter',\n",
       " 'Compose',\n",
       " 'FiveCrop',\n",
       " 'Grayscale',\n",
       " 'Lambda',\n",
       " 'LinearTransformation',\n",
       " 'Normalize',\n",
       " 'Pad',\n",
       " 'RandomAffine',\n",
       " 'RandomApply',\n",
       " 'RandomChoice',\n",
       " 'RandomCrop',\n",
       " 'RandomErasing',\n",
       " 'RandomGrayscale',\n",
       " 'RandomHorizontalFlip',\n",
       " 'RandomOrder',\n",
       " 'RandomPerspective',\n",
       " 'RandomResizedCrop',\n",
       " 'RandomRotation',\n",
       " 'RandomSizedCrop',\n",
       " 'RandomVerticalFlip',\n",
       " 'Resize',\n",
       " 'Scale',\n",
       " 'TenCrop',\n",
       " 'ToPILImage',\n",
       " 'ToTensor',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'functional',\n",
       " 'transforms']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "dir(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among those transforms, we can spot ToTensor, which turns NumPy arrays and PIL\n",
    "images to tensors. It also takes care to lay out the dimensions of the output tensor as\n",
    "C × H × W (channel, height, width; just as we covered in chapter 4).\n",
    "\n",
    " Let’s try out the ToTensor transform. Once instantiated, it can be called like a\n",
    "function with the PIL image as the argument, returning a tensor as output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_tensor = transforms.ToTensor()\n",
    "img_t = to_tensor(img)\n",
    "img_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image has been turned into a 3 × 32 × 32 tensor and therefore a 3-channel (RGB)\n",
    "32 × 32 image. Note that nothing has happened to label; it is still an integer.\n",
    "\n",
    "\n",
    " As we anticipated, we can pass the transform directly as an argument to dataset\n",
    ".CIFAR10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_cifar10 = datasets.CIFAR10(data_path, train=True, download=False,\n",
    "                                 transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now our images will come out as tensors\n",
    "img_t, _ = tensor_cifar10[99]\n",
    "type(img_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 32, 32]), torch.float32)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#As expected CxHxW and float 32\n",
    "img_t.shape, img_t.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas the values in the original PIL image ranged from 0 to 255 (8 bits per channel), the ToTensor transform turns the data into a 32-bit floating-point per channel,\n",
    "scaling the values down from 0.0 to 1.0. \n",
    "\n",
    "Let’s verify that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(1.))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t.min(), img_t.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets verify its the same image, note we need to change the axis\n",
    "plt.imshow(img_t.permute(1,2,0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.4 Normalizing data\n",
    "\n",
    "Transforms are really handy because we can chain them using transforms.Compose,\n",
    "and they can handle normalization and data augmentation transparently, directly in\n",
    "the data loader. For instance, it’s good practice to normalize the dataset so that each\n",
    "channel has zero mean and unitary standard deviation. We mentioned this in chapter\n",
    "4, but now, after going through chapter 5, we also have an intuition for why: by choosing\n",
    "activation functions that are linear around 0 plus or minus 1 (or 2), keeping the data\n",
    "in the same range means it’s more likely that neurons have nonzero gradients and, hence will learn faster\n",
    "\n",
    "Also, normalizing each channel so that it has the same\n",
    "distribution will ensure that channel information can be mixed and updated through\n",
    "gradient descent using the same learning rate. This is just like the situation in section\n",
    "5.4.4 when we rescaled the weight to be of the same magnitude as the bias in our\n",
    "temperature-conversion model.\n",
    "\n",
    "\n",
    " In order to make it so that each channel has zero mean and unitary standard deviation, we can compute the mean value and the standard deviation of each channel\n",
    "across the dataset and apply the following transform: v_n[c] = (v[c] - mean[c]) /\n",
    "stdev[c]. This is what transforms.Normalize does. The values of mean and stdev\n",
    "must be computed offline (they are not computed by the transform).\n",
    "\n",
    "\n",
    "__Since CIFAR is small, we can manipulate it entirely in memory__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32, 50000])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs = torch.stack([img_t for img_t, _ in tensor_cifar10], dim=3)\n",
    "imgs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that view(3, -1) keeps the three channels and\n",
    "merges all the remaining dimensions into one, figuring\n",
    "out the appropriate size. Here our 3 × 32 × 32 image is\n",
    "transformed into a 3 × 1,024 vector, and then the mean\n",
    "is taken over the 1,024 elements of each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4915, 0.4823, 0.4468]), tensor([0.2470, 0.2435, 0.2616]))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we can easily compute the mean per channel and std\n",
    "means = imgs.view(3,-1).mean(dim=1)\n",
    "\n",
    "stds = imgs.view(3,-1).std(dim=1)\n",
    "\n",
    "means, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalize(mean=(0.4915, 0.4823, 0.4468), std=(0.247, 0.2435, 0.2616))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#With these numbers we can now normalize our channels\n",
    "transforms.Normalize((0.4915, 0.4823, 0.4468),(0.2470, 0.2435, 0.2616))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we just need to add it when instantiating our dataset\n",
    "transformed_CIFAR = datasets.CIFAR10(data_path, train=True, download=False,\n",
    "                                    transform=transforms.Compose([\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                                                             (0.2470, 0.2435, 0.2616))\n",
    "                                    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, at this point, plotting an image drawn from the dataset won’t provide us\n",
    "with a faithful representation of the actual image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11ea6c250>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_t, _ = transformed_CIFAR[99]\n",
    "plt.imshow(img_t.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is because normalization\n",
    "has shifted the RGB levels outside the 0.0 to 1.0 range and changed the overall magnitudes of the channels. All of the data is still there; it’s just that Matplotlib renders it as\n",
    "black. We’ll keep this in mind for the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2 Distinguishing birds from airplanes\n",
    " We’ll pick out all the birds and airplanes from our\n",
    "CIFAR-10 dataset and build a neural network that can tell birds and airplanes apart.\n",
    "\n",
    "## 7.2.1 Building the Dataset\n",
    "\n",
    "The first step is to get the data in the right shape. We could create a Dataset subclass\n",
    "that only includes birds and airplanes. However, the dataset is small, and we only need\n",
    "indexing and len to work on our dataset. It doesn’t actually have to be a subclass of\n",
    "torch.utils.data.dataset.Dataset! Well, why not take a shortcut and just filter the\n",
    "data in cifar10 and remap the labels so they are contiguous? Here’s how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0:0, 2:1}\n",
    "class_names = ['airplane', 'bird']\n",
    "\n",
    "cifar2 = [(img, label_map[label]) for img, label in cifar10 if label in [0,2]]\n",
    "\n",
    "cifar2_val = [(img, label_map[label]) for img,label in cifar10_val if label in [0,2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cifar2 object satisfies the basic requirements for a Dataset—that is, __len__ and\n",
    "__getitem__ are defined—so we’re going to use that. We should be aware, however,\n",
    "that this is a clever shortcut and we might wish to implement a proper Dataset if we\n",
    "hit limitations with it.4\n",
    "\n",
    "## 7.2.2 A Fully Connected Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*32*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets build a model\n",
    "n_out = 2\n",
    "\n",
    "model = nn.Sequential(\n",
    "                    nn.Linear(3072, 512), #Use 3072 since we flatten the image(32*32*3 = 3072)\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(512, n_out)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2.3 Output of a classifier\n",
    "\n",
    "We need to recognize that the output is categorical: The key realization in this case is that we can interpret our output as probabilities: the\n",
    "first entry is the probability of “airplane,” and the second is the probability of “bird.”\n",
    " Casting the problem in terms of probabilities imposes a few extra constraints on\n",
    "the outputs of our network:\n",
    "- Each element of the output must be in the [0.0, 1.0] range (a probability of\n",
    "an outcome cannot be less than 0 or greater than 1).\n",
    "- The elements of the output must add up to 1.0 (we’re certain that one of the\n",
    "two outcomes will occur).\n",
    "\n",
    "The solution is a simple and differentiable activation called __Softmax__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2.4 Representing the output as probabilities\n",
    "Softmax is a function that takes a vector of values and produces another vector of the\n",
    "same dimension, where the values satisfy the constraints we just listed to represent\n",
    "probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0900, 0.2447, 0.6652]), tensor(1.))"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.,2.,3.])\n",
    "softmax(x), softmax(x).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax is a monotone function, in that lower values in the input will correspond to\n",
    "lower values in the output. However, it’s not scale invariant, in that the ratio between\n",
    "values is not preserved. In fact, the ratio between the first and second elements of the\n",
    "input is 0.5, while the ratio between the same elements in the output is 0.3678. This is\n",
    "not a real issue, since the learning process will drive the parameters of the model in a\n",
    "way that values have appropriate ratios.\n",
    "\n",
    "Can call softmax in pytorch using __nn.Softmax()__\n",
    "\n",
    " Since, as usual, input tensors\n",
    "may have an additional batch 0th dimension, or have dimensions along which they\n",
    "encode probabilities and others in which they don’t, nn.Softmax requires us to specify\n",
    "the dimension along which the softmax function is applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0900, 0.2447, 0.6652],\n",
       "        [0.0900, 0.2447, 0.6652]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "x = torch.tensor([[1.,2.,3.],\n",
    "                [1.,2.,3.]])\n",
    "\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can add softmax to our model\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512,2),\n",
    "            nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11f465490>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets try out our model before we train\n",
    "img, _ = cifar2[0]\n",
    "plt.imshow(img.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We recall that our model expects 3,072 features in the input, and that nn\n",
    "works with data organized into batches along the zeroth dimension. So we need to\n",
    "turn our 3 × 32 × 32 image into a 1D tensor and then add an extra dimension in the\n",
    "zeroth position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_batch = img.view(-1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4678, 0.5322]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now lets use the model\n",
    "out = model(img_batch)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got probabilities but they are random guesses since our model isnt trained yet.\n",
    "\n",
    "\n",
    "Interestingly, we also see grad_fn for the\n",
    "output, which is the tip of the backward computation graph (it will be used as soon as\n",
    "we need to backpropagate)\n",
    "\n",
    " In addition, while we know which output probability is supposed to be which\n",
    "(recall our class_names), our network has no indication of that. Is the first entry “airplane” and the second “bird,” or the other way around? The network can’t even tell\n",
    "that at this point. It’s the loss function that associates a meaning with these two numbers, after backpropagation. If the labels are provided as index 0 for “airplane” and\n",
    "index 1 for “bird,” then that’s the order the outputs will be induced to take. Thus,\n",
    "after training, we will be able to get the label as an index by computing the argmax of\n",
    "the output probabilities: that is, the index at which we get the maximum probability.\n",
    "Conveniently, when supplied with a dimension, torch.max returns the maximum element along that dimension as well as the index at which that value occurs. In our case,\n",
    "we need to take the max along the probability vector (not across batches), therefore,\n",
    "dimension 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, index = torch.max(out, dim=1)\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2.5 A loss for classifying\n",
    "\n",
    " We could still use MSE and make our\n",
    "output probabilities converge to [0.0, 1.0] and [1.0, 0.0]. However, thinking about\n",
    "it, we’re not really interested in reproducing these values exactly. Looking back at the\n",
    "argmax operation we used to extract the index of the predicted class, what we’re really\n",
    "interested in is that the first probability is higher than the second for airplanes and vice\n",
    "versa for birds. In other words, we want to penalize misclassifications rather than painstakingly penalize everything that doesn’t look exactly like a 0.0 or 1.0.\n",
    "\n",
    " What we need to maximize in this case is the probability associated with the correct\n",
    "class, out[class_index], where out is the output of softmax and class_index is a vector containing 0 for “airplane” and 1 for “bird” for each sample. This quantity—that\n",
    "is, the probability associated with the correct class—is referred to as the likelihood (of\n",
    "our model’s parameters, given the data).8\n",
    " In other words, we want a loss function that\n",
    "is very high when the likelihood is low: so low that the alternatives have a higher probability. Conversely, the loss should be low when the likelihood is higher than the alternatives, and we’re not really fixated on driving the probability up to 1.\n",
    "\n",
    "There’s a loss function that behaves that way, and it’s called __negative log likelihood\n",
    "(NLL)__. It has the expression __NLL = - sum(log(out_i[c_i]))__, where the sum is taken\n",
    "over N samples and c_i is the correct class for sample i.\n",
    "\n",
    "\n",
    " when low probabilities are assigned to the data, the NLL grows\n",
    "to infinity, whereas it decreases at a rather shallow rate when probabilities are greater\n",
    "than 0.5. Remember that the NLL takes probabilities as input; so, as the likelihood\n",
    "grows, the other probabilities will necessarily decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how do we do this in PyTorch? \n",
    "\n",
    "PyTorch has an __nn.NLLLoss class__. However, as opposed to what you might expect, it does not take probabilities but rather takes a tensor of log probabilities as input. It then computes the NLL of our model given the batch of data. There’s a good reason behind the input convention: taking the logarithm of a probability is tricky when the probability gets close to zero. __The workaround is to use nn.LogSoftmax instead of nn.Softmax__, which takes care to make the calculation numerically stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "                nn.Linear(3072, 512),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(512, 2),\n",
    "                nn.LogSoftmax(dim=1))\n",
    "\n",
    "loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5032, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we can test the NLLLoss on our model\n",
    "img, label = cifar2[0]\n",
    "out = model(img.view(-1).unsqueeze(0))\n",
    "\n",
    "loss(out, torch.tensor([label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2.6 Training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "                nn.Linear(3072, 512),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(512, 2),\n",
    "                nn.LogSoftmax(dim=1))\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr= learning_rate)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for img, label in cifar2:\n",
    "        out = model(img.view(-1).unsqueeze(0))\n",
    "        loss = loss_fn(out, torch.tensor([label]))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " By shuffling samples at each epoch and estimating the gradient on one or (preferably, for stability) a few\n",
    "samples at a time, we are effectively introducing randomness in our gradient descent.\n",
    "It turns out that following gradients estimated over minibatches, which are poorer approximations of gradients\n",
    "estimated across the whole dataset, helps convergence and prevents the optimization\n",
    "process from getting stuck in local minima it encounters along the way.\n",
    "\n",
    "\n",
    "Typically, minibatches are a constant size that we need to set prior to training, just\n",
    "like the learning rate. These are called hyperparameters, to distinguish them from the\n",
    "parameters of a model.\n",
    "\n",
    " The torch.utils.data module has a class that helps with shuffling and\n",
    "organizing the data in minibatches: DataLoader. The job of a data loader is to sample\n",
    "minibatches from a dataset, giving us the flexibility to choose from different sampling\n",
    "strategies. A very common strategy is uniform sampling after shuffling the data at each\n",
    "epoch.\n",
    "\n",
    "\n",
    "Let’s see how this is done. At a minimum, the DataLoader constructor takes a Dataset\n",
    "object as input, along with batch_size and a shuffle Boolean that indicates whether\n",
    "the data needs to be shuffled at the beginning of each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.432117\n",
      "Epoch: 1, Loss: 0.379650\n",
      "Epoch: 2, Loss: 0.352264\n",
      "Epoch: 3, Loss: 0.434213\n",
      "Epoch: 4, Loss: 0.456035\n",
      "Epoch: 5, Loss: 0.351303\n",
      "Epoch: 6, Loss: 0.304718\n",
      "Epoch: 7, Loss: 0.376707\n",
      "Epoch: 8, Loss: 0.551254\n",
      "Epoch: 9, Loss: 0.120805\n"
     ]
    }
   ],
   "source": [
    "#A dataloader can be iterated over so we can use it in our loop\n",
    "model = nn.Sequential(\n",
    "                nn.Linear(3072, 512),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(512, 2),\n",
    "                nn.LogSoftmax(dim=1))\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr= learning_rate)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size,-1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss))) #due to shuffling it prints loss for random batch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the loss decreases somehow, but we have no idea whether it’s low enough.\n",
    "Since our goal here is to correctly assign classes to images, and preferably do that on\n",
    "an independent dataset, we can compute the accuracy of our model on the validation\n",
    "set in terms of the number of correct classifications over the total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size,-1))\n",
    "        _, predicted = torch.max(outputs,dim=1)\n",
    "        total+=labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1630)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a great performance, but quite a lot better than random. In our defense, our\n",
    "model was quite a shallow classifier; lets try something a bit more complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "        nn.Linear(3072, 1024),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(1024, 512),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(512, 128),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(128, 2),\n",
    "        nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combination of nn.LogSoftmax and nn.NLLLoss is equivalent to using\n",
    "nn.CrossEntropyLoss. This terminology is a particularity of PyTorch, as the\n",
    "nn.NLLoss computes, in fact, the cross entropy but with log probability predictions as\n",
    "inputs where nn.CrossEntropyLoss takes scores (sometimes called logits). Technically, nn.NLLLoss is the cross entropy between the Dirac distribution, putting all mass\n",
    "on the target, and the predicted distribution given by the log probability inputs.\n",
    " \n",
    " \n",
    " To add to the confusion, in information theory, up to normalization by sample size,\n",
    "this cross entropy can be interpreted as a negative log likelihood of the predicted distribution under the target distribution as an outcome. So both losses are the negative\n",
    "log likelihood of the model parameters given the data when our model predicts the\n",
    "(softmax-applied) probabilities. In this book, we won’t rely on these details, but don’t\n",
    "let the PyTorch naming confuse you when you see the terms used in the literature.\n",
    " It is quite common to drop the last nn.LogSoftmax layer from the network and use\n",
    "nn.CrossEntropyLoss as a loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 2))\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the numbers will be exactly the same as with nn.LogSoftmax and nn.NLLLoss.\n",
    "It’s just more convenient to do it all in one pass, with the only gotcha being that the output of our model will not be interpretable as probabilities (or log probabilities). We’ll\n",
    "need to explicitly pass the output through a softmax to obtain those.\n",
    "\n",
    "Training this model and evaluating the accuracy on the validation set (0.802000)\n",
    "lets us appreciate that a larger model bought us an increase in accuracy, but not that\n",
    "much. The accuracy on the training set is practically perfect (0.998100). What is this\n",
    "telling us? That we are overfitting our model in both cases. Our fully connected\n",
    "model is finding a way to discriminate birds and airplanes on the training set by memorizing the training set, but performance on the validation set is not all that great,\n",
    "even if we choose a larger mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out how many elements are in each tensor\n",
    "instance, we can call the numel method. Summing those gives us our total count.\n",
    "Depending on our use case, counting parameters might require us to check whether a\n",
    "parameter has requires_grad set to True, as well. We might want to differentiate the\n",
    "number of trainable parameters from the overall model size. Let’s take a look at what\n",
    "we have right now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3737474, [3145728, 1024, 524288, 512, 65536, 128, 256, 2])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_list = [p.numel() for p in model.parameters() if p.requires_grad == True]\n",
    "\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is this telling us? That our neural network won’t scale very well with the number\n",
    "of pixels. What if we had a 1,024 × 1,024 RGB image? That’s 3.1 million input values.\n",
    "Even abruptly going to 1,024 hidden features (which is not going to work for our classifier), we would have over 3 billion parameters. Using 32-bit floats, we’re already at 12\n",
    "GB of RAM, and we haven’t even hit the second layer, much less computed and stored\n",
    "the gradients. That’s just not going to fit on most present-day GPUs.\n",
    "\n",
    "\n",
    "## 7.2.7 The limits of going fully connected\n",
    " On one hand, we are allowing for the combination of any pixel with every other pixel in the image being potentially relevant for our\n",
    "task. On the other hand, we aren’t utilizing the relative position of neighboring or faraway pixels, since we are treating the image as one big vector of numbers\n",
    "\n",
    "A fully connected network is not translation invariant. This means a\n",
    "network that has been trained to recognize a Spitfire starting at position 4,4 will not\n",
    "be able to recognize the exact same Spitfire starting at position 8,8. We would then have\n",
    "to augment the dataset—that is, apply random translations to images during training—\n",
    "so the network would have a chance to see Spitfires all over the image, and we would\n",
    "need to do this for every image in the dataset  However, this data\n",
    "augmentation strategy comes at a cost: the number of hidden features—that is, of\n",
    "parameters—must be large enough to store the information about all of these translated replicas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:facial_rec]",
   "language": "python",
   "name": "conda-env-facial_rec-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
