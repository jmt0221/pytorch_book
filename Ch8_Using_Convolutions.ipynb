{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:3em;\">Ch8: Using convolutions to Generalize </span>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data-unversioned/p1ch7/'\n",
    "cifar10 = torchvision.datasets.CIFAR10(data_path, train=True, download=False,\n",
    "                                       transform=transforms.Compose([\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                                             (0.2470, 0.2435, 0.2616))\n",
    "                                       ]))\n",
    "cifar10_val = torchvision.datasets.CIFAR10(data_path, train=False, download=False,\n",
    "                                       transform=transforms.Compose([\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                                             (0.2470, 0.2435, 0.2616))\n",
    "                                       ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10\n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, _ = cifar2[0]\n",
    "\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Due to the fully connected setup needed to detect the various possible translations of the bird or\n",
    "airplane in the image, we have both too many parameters (making it easier for the\n",
    "model to memorize the training set) and no position independence (making it\n",
    "harder to generalize). There is a solution and it consists of replacing the dense, fully connected affine transformation in our neural network unit with a different linear operation: convolution.\n",
    "\n",
    "# 8.1 The case for convolutions\n",
    "\n",
    "We said earlier that taking a 1D view of our input image and multiplying it by an\n",
    "n_output_features × n_input_features weight matrix, as is done in nn.Linear,\n",
    "means for each channel in the image, computing a weighted sum of all the pixels multiplied by a set of weights, one per output feature.\n",
    "\n",
    " We also said that, if we want to recognize patterns corresponding to objects, like an\n",
    "airplane in the sky, we will likely need to look at how nearby pixels are arranged, and\n",
    "we will be less interested in how pixels that are far from each other appear in combination. Essentially, it doesn’t matter if our image of a Spitfire has a tree or cloud or\n",
    "kite in the corner or not.\n",
    "\n",
    " In order to translate this intuition into mathematical form, we could compute the\n",
    "weighted sum of a pixel with its immediate neighbors, rather than with all other pixels\n",
    "in the image. This would be equivalent to building weight matrices, one per output\n",
    "feature and output pixel location, in which all weights beyond a certain distance from\n",
    "a center pixel are zero. This will still be a weighted sum: that is, a linear operation\n",
    "\n",
    "\n",
    "## 8.1.1 What convolutions do\n",
    "\n",
    "For other weights, we would have to find a way to keep\n",
    "entries in sync that correspond to the same relative position of input and output pixels.\n",
    "This means we would need to initialize them to the same values and ensure that all these\n",
    "tied weights stayed the same while the network is updated during training. This way, we\n",
    "would ensure that weights operate in neighborhoods to respond to local patterns, and\n",
    "local patterns are identified no matter where they occur in the image.\n",
    "\n",
    "Of course, this approach is more than impractical. Fortunately, there is a readily\n",
    "available, local, translation-invariant linear operation on the image: a __convolution.__\n",
    "\n",
    "__Convolution, or more precisely, discrete convolution__\n",
    " (there’s an analogous continuous version that we won’t go into here), is defined for a 2D image as the scalar product of a weight matrix, the kernel, with every neighborhood in the input\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, we “translate” the kernel on the i11 location of the input image, and we\n",
    "multiply each weight by the value of the input image at the corresponding location.\n",
    "Thus, the output image is created by translating the kernel on all input locations and\n",
    "performing the weighted sum. For a multichannel image, like our RGB image, the\n",
    "weight matrix would be a 3 × 3 × 3 matrix: one set of weights for every channel, contributing together to the output values.\n",
    "\n",
    " __Note__ that, just like the elements in the weight matrix of nn.Linear, the weights in\n",
    "the kernel are not known in advance, but they are initialized randomly and updated\n",
    "through backpropagation. \n",
    "\n",
    "__Note__ also that the same kernel, and thus each weight in the\n",
    "kernel, is reused across the whole image. Thinking back to autograd, this means the use\n",
    "of each weight has a history spanning the entire image. Thus, the derivative of the loss\n",
    "with respect to a convolution weight includes contributions from the entire image.\n",
    "\n",
    "\n",
    "\n",
    "It’s now possible to see the connection to what we were stating earlier: a convolution is\n",
    "equivalent to having multiple linear operations whose weights are zero almost everywhere except around individual pixels and that receive equal updates during training.\n",
    " Summarizing, by switching to convolutions, we get\n",
    "-  Local operations on neighborhoods\n",
    "-  Translation invariance\n",
    "-  Models with a lot fewer parameters\n",
    "\n",
    "# 8.2 Convolutions in action\n",
    "\n",
    " The torch.nn module provides convolutions for 1, 2, and 3 dimensions:\n",
    " - nn.Conv1d for time series, \n",
    " - nn.Conv2d for images,\n",
    " - nn.Conv3d for volumes or videos.\n",
    " \n",
    "At a minimum, the arguments we\n",
    "provide to nn.Conv2d are the number of input features (or channels, since we’re dealing with multichannel images: that is, more than one value per pixel), the number of output\n",
    "features, and the size of the kernel. For instance, for our first convolutional module,\n",
    "we’ll have 3 input features per pixel (the RGB channels) and an arbitrary number of\n",
    "channels in the output—say, 16. The more channels in the output image, the more the\n",
    "capacity of the network. We need the channels to be able to detect many different types\n",
    "of features. Also, because we are randomly initializing them, some of the features we’ll\n",
    "get, even after training, will turn out to be useless.2\n",
    " Let’s stick to a kernel size of 3 × 3.\n",
    "\n",
    "\n",
    "\n",
    "It is very common to have kernel sizes that are the same in all directions, so\n",
    "PyTorch has a shortcut for this: whenever kernel_size=3 is specified for a 2D convolution, it means 3 × 3 (provided as a tuple (3, 3) in Python). For a 3D convolution, it\n",
    "means 3 × 3 × 3. \n",
    "\n",
    "The CT scans we will see in part 2 of the book have a different voxel\n",
    "(volumetric pixel) resolution in one of the three axes. In such a case, it makes sense to\n",
    "consider kernels that have a different size for the exceptional dimension. But for now,\n",
    "we stick with having the same size of convolutions across all dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3,16,kernel_size=3)\n",
    "conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we expect to be the shape of the weight tensor? \n",
    "\n",
    "The kernel is of size 3 × 3, so\n",
    "we want the weight to consist of 3 × 3 parts. \n",
    "\n",
    "For a single output pixel value, our kernel\n",
    "would consider, say, in_ch = 3 input channels, so the weight component for a single\n",
    "output pixel value (and by translation the invariance for the entire output channel) is\n",
    "of shape in_ch × 3 × 3. \n",
    "\n",
    "Finally, we have as many of those as we have output channels,\n",
    "here out_ch = 16, so the complete weight tensor is out_ch × in_ch × 3 × 3, in our case\n",
    "16 × 3 × 3 × 3. The bias will have size 16 (we haven’t talked about bias for a while for\n",
    "simplicity, but just as in the linear module case, it’s a constant value we add to each\n",
    "channel of the output image). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.shape, conv.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A 2D convolution pass produces a 2D image as output, whose pixels are a weighted\n",
    "sum over neighborhoods of the input image. As usual, we need to add the zeroth batch dimension with\n",
    "unsqueeze if we want to call the conv module with one input image, since nn.Conv2d\n",
    "expects a B × C × H × W shaped tensor as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11da9f650>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We’re curious, so we can display the output \n",
    "plt.imshow(output[0][0].detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.1 Padding the boundary\n",
    "\n",
    "If we look at the shape of the output, we can see that the shape has gone from 32x32 to 30x30. This is because the middle of the kernel starts at the top left pixel, so we need to add a boundary/padding to the outside so it can capture the entire picture\n",
    "\n",
    "By default, PyTorch willslide the convolution kernel within the input picture, getting width - kernel_width +1 horizontal and vertical positions. For odd-sized kernels, this results in images that are one-half the convolution kernel’s width (in our case, 3//2 = 1) smaller on each side. PyTorch gives us the possibility of padding the image by creating ghost pixels around the border that have value zero as far as the convolution is concerned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 32, 32]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3,16,kernel_size=3,padding=1)\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the sizes of weight and bias don’t change, regardless of whether padding is\n",
    "used\n",
    "\n",
    "__There are two main reasons to pad convolutions.__ \n",
    "\n",
    "- First, doing so helps us separate\n",
    "the matters of convolution and changing image sizes, so we have one less thing to\n",
    "remember.\n",
    "- Second, when we have more elaborate structures such as skip connections (discussed in section 8.5.3) or the U-Nets we’ll cover in part 2, we want the\n",
    "tensors before and after a few convolutions to be of compatible size so that we can\n",
    "add them or take differences. \n",
    "\n",
    "## 8.2.2 Detecting features with convolutions\n",
    "\n",
    "We said earlier that weight and bias are parameters that are learned through backpropagation, exactly as it happens for weight and bias in nn.Linear. However, we can\n",
    "play with convolution by setting weights by hand and see what happens.\n",
    " Let’s first zero out bias, just to remove any confounding factors, and then set\n",
    "weights to a constant value so that each pixel in the output gets the mean of its neighbors. For each 3 × 3 neighborhood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    conv.bias.zero_()\n",
    "    \n",
    "with torch.no_grad():\n",
    "    conv.weight.fill_(1.0/9.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11c210f10>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#all the pixels is the average of a neighborhood in the original image\n",
    "output = conv(img.unsqueeze(0))\n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11de62cd0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets try another kernel\n",
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "with torch.no_grad():\n",
    "    conv.weight[:] = torch.tensor([[-1.0, 0.0, 1.0],\n",
    "                                    [-1.0, 0.0, 1.0],\n",
    "                                    [-1.0, 0.0, 1.0]])\n",
    "    \n",
    "output = conv(img.unsqueeze(0))\n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the kernel is applied on a vertical boundary between two adjacent regions\n",
    "of different intensity, output of that region will have a high value. If the kernel is applied on a region of\n",
    "uniform intensity, output will be zero. __It’s an edge-detection kernel:__ the kernel highlights the\n",
    "vertical edge between two horizontally adjacent regions.\n",
    "\n",
    " \n",
    " \n",
    "  With deep learning, we let kernels be estimated from data in whatever way the discrimination is most effective: for instance, in terms of minimizing the negative crossentropy loss between the output and the ground truth that we introduced in section\n",
    "7.2.5. From this angle, the job of a convolutional neural network is to estimate the kernel of a set of filter banks in successive layers that will transform a multichannel image\n",
    "into another multichannel image, where different channels correspond to different\n",
    "features (such as one channel for the average, another channel for vertical edges, and\n",
    "so on).\n",
    "\n",
    "\n",
    "## 8.2.3 Looking further with depth and pooling\n",
    "\n",
    "We recommended the use of small kernels, like 3 × 3, or 5 × 5: that’s peak locality, all right. What about the big picture? How do we know that all structures in our images are 3 pixels or 5 pixels wide? Well, we don’t, because they aren’t. And if they aren’t, how are our networks going to be equipped to see those patterns with larger scope? \n",
    "\n",
    "\n",
    "- One possibility could be to use large convolution kernels. Well, sure, at the limit we\n",
    "could get a 32 × 32 kernel for a 32 × 32 image, but we would converge to the old fully\n",
    "connected, affine transformation and lose all the nice properties of convolution.\n",
    "\n",
    "\n",
    "- Another option, which is used in convolutional neural networks, is stacking one convolution after the other and at the same time downsampling the image between successive convolutions.\n",
    "\n",
    "\n",
    "__DOWNSAMPLING__\n",
    "\n",
    "Downsampling could in principle occur in different ways. How we compute the value of the output based on the values of the input is up to us. We could:\n",
    "\n",
    "- __Average the four pixels__. This __average pooling__ was a common approach early on but\n",
    "has fallen out of favor somewhat.\n",
    "\n",
    "\n",
    "- __Take the maximum of the four pixels__. This approach, called __max pooling__, is currently\n",
    "the most commonly used approach, but it has a downside of discarding the\n",
    "other three-quarters of the data.\n",
    "\n",
    "\n",
    "- __Perform a strided convolution, where only every Nth pixel is calculated__. A 3 × 4 convolution with stride 2 still incorporates input from all pixels from the previous layer.\n",
    "The literature shows promise for this approach, but it has not yet supplanted\n",
    "max pooling.\n",
    "\n",
    "\n",
    "We will be focusing on max pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, the output images from a convolution layer, especially since they are followed by an activation just like any other linear layer, tend to have a high magnitude where certain features corresponding to the estimated kernel are detected (such as\n",
    "vertical lines).\n",
    "\n",
    "By keeping the highest value in the 2 × 2 neighborhood as the downsampled output, we ensure that the features that are found survive the downsampling,\n",
    "at the expense of the weaker responses.\n",
    "\n",
    "Max pooling is provided by the __nn.MaxPool2d module__ (as with convolution, there are\n",
    "versions for 1D and 3D data). It takes as input the size of the neighborhood over which\n",
    "to operate the pooling operation. If we wish to downsample our image by half, we’ll want\n",
    "to use a size of 2, since MaxPool2d uses a stride of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 16, 16]), torch.Size([1, 3, 32, 32]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = nn.MaxPool2d(2)\n",
    "pool(output).shape, img.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where we use multiple layers of convolutions and pooling, say 2, the first set of kernels operates on small neighborhoods on firstorder, low-level features, while the second set of kernels effectively operates on wider\n",
    "neighborhoods, producing features that are compositions of the previous features.\n",
    "This is a very powerful mechanism that provides convolutional neural networks with\n",
    "the ability to see into very complex scenes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "    \n",
    "            # ...  Something is missing here\n",
    "    \n",
    "            nn.Linear(8 * 8 * 8, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16384"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8*8*8*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets check how many parameters and ignore the missing portion\n",
    "params = [p.numel() for p in model.parameters()]\n",
    "sum(params), params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.4 Putting it all together for our network\n",
    "\n",
    "What’s missing there is the reshaping step from an 8-channel 8 × 8 image to a 512-\n",
    "element, 1D vector (1D if we ignore the batch dimension, that is). This could be\n",
    "achieved by calling view on the output of the last nn.MaxPool2d, but unfortunately, we\n",
    "don’t have any explicit visibility of the output of each module when we use\n",
    "nn.Sequential.\n",
    "\n",
    "# 8.3 Subclassing nn.Module\n",
    "At some point in developing neural networks, we will find ourselves in a situation where\n",
    "we want to compute something that the premade modules do not cover. Here, it is something very simple like reshaping, ; but in section 8.5.3, we use the same construction to implement residual connections. So in this section, we learn how to make our own nn.Module subclasses that we can then use just like the prebuilt ones or nn.Sequential.\n",
    " \n",
    " When we want to build models that do more complex things than just applying one layer after another, we need to leave nn.Sequential for something that gives us added flexibility. PyTorch allows us to use any computation in our model by subclassing nn.Module.\n",
    " \n",
    " In order to subclass nn.Module, at a minimum we need to define a forward function\n",
    "that takes the inputs to the module and returns the output. This is where we define our\n",
    "module’s computation. The name forward here is reminiscent of a distant past, when\n",
    "modules needed to define both the forward and backward passes we met in section\n",
    "5.5.1. With PyTorch, if we use standard torch operations, autograd will take care of the\n",
    "backward pass automatically; and indeed, an nn.Module never comes with a backward.\n",
    "\n",
    "Typically, our computation will use other modules—premade like convolutions or\n",
    "customized. To include these submodules, we typically define them in the constructor\n",
    "__init__ and assign them to self for use in the forward function. They will, at the\n",
    "same time, hold their parameters throughout the lifetime of our module. Note that you\n",
    "need to call super().__  init  ___ () before you can do that (or PyTorch will remind you).\n",
    "\n",
    "## 8.3.1 Our network as an nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3,16, kernel_size=3,padding=1)\n",
    "        self.act1 = nn.Tanh(),\n",
    "        self.pool1 = nn.MaxPool2d(2),\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.Tanh(),\n",
    "        self.pool2 = nn.MaxPool2d(2),\n",
    "        self.fc1 = nn.Linear(8*8*8, 32)\n",
    "        self.act3 = nn.Tanh(),\n",
    "        self.fc2 = nn.Linear(32,2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv1(out)))\n",
    "        out = out.view(-1, 8*8*8)\n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Net class is equivalent to the nn.Sequential modelwe built earlier in terms of submodules; but by writing\n",
    "the forward function explicitly, we can manipulate the output of self.pool3 directly and call view on it to turn it into a B × N vector.\n",
    "\n",
    "Here we use a subclass of nn.Module to contain our entire model. We could also use subclasses to define new building blocks for more complex networks\n",
    "\n",
    "\n",
    "__First__\n",
    "- our goal is reflected by the size of our intermediate values generally\n",
    "shrinking—this is done by reducing the number of channels in the convolutions, by\n",
    "reducing the number of pixels through pooling, and by having an output dimension\n",
    "lower than the input dimension in the linear layers. This is a common trait of\n",
    "classification networks. However, in many popular architectures like the ResNets we saw\n",
    "in chapter 2 and discuss more in section 8.5.3, the reduction is achieved by pooling in\n",
    "the spatial resolution, but the number of channels increases (still resulting in a\n",
    "reduction in size). It seems that our pattern of fast information reduction works well\n",
    "with networks of limited depth and small images; but for deeper networks, the decrease\n",
    "is typically slower\n",
    "\n",
    "__Second__\n",
    "\n",
    "-  In one layer, there is not a reduction of output size with regard to input\n",
    "size: the initial convolution. If we consider a single output pixel as a vector of 32 elements (the channels), it is a linear transformation of 27 elements (as a convolution of\n",
    "3 channels × 3 × 3 kernel size)—only a moderate increase. In ResNet, the initial convolution generates 64 channels from 147 elements (3 channels × 7 × 7 kernel size).6\n",
    "So the first layer is exceptional in that it greatly increases the overall dimension (as in\n",
    "channels times pixels) of the data flowing through it, but the mapping for each output pixel considered in isolation still has approximately as many outputs as inputs.7\n",
    "\n",
    "\n",
    "\n",
    "## 8.3.2 How PyTorch keeps track of parameters and submodules\n",
    "\n",
    "Interestingly, assigning an instance of nn.Module to an attribute in an nn.Module, as\n",
    "we did in the earlier constructor, automatically registers the module as a submodule.\n",
    "\n",
    "- ___NOTE__ The submodules must be top-level attributes, not buried inside list or\n",
    "dict instances! Otherwise the optimizer will not be able to locate the submodules (and, hence, their parameters). For situations where your model\n",
    "requires a list or dict of submodules, PyTorch provides nn.ModuleList and\n",
    "nn.ModuleDict.\n",
    "\n",
    "\n",
    "\n",
    "__We can call arbitrary methods of an nn.Module subclass.__ For example, for a model\n",
    "where training is substantially different than its use, say, for prediction, it may make\n",
    "sense to have a predict method. Be aware that calling such methods will be similar to\n",
    "calling forward instead of the module itself—they will be ignorant of hooks, and the\n",
    "JIT does not see the module structure when using them because we are missing the\n",
    "equivalent of the __call__ bits shown in section 6.2.1.\n",
    "\n",
    " This allows Net to have access to the parameters of its submodules without further\n",
    "action by the user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens here is that the parameters() call delves into all submodules assigned\n",
    "as attributes in the constructor and recursively calls parameters() on them. No matter how nested the submodule, any nn.Module can access the list of all child parameters. By accessing their grad attribute, which has been populated by autograd, the optimizer will know how to change parameters to minimize the loss. \n",
    "\n",
    "\n",
    "\n",
    "Looking back at the implementation of the Net class, and thinking about the\n",
    "utility of registering submodules in the constructor so that we can access their parameters, it appears a bit of a waste that we are also registering submodules that have no\n",
    "parameters, like nn.Tanh and nn.MaxPool2d. Wouldn’t it be easier to call these\n",
    "directly in the forward function, just as we called view?\n",
    "\n",
    "## 8.3.3 The functional API\n",
    "\n",
    "It sure would! And that’s why PyTorch has functional counterparts for every nn module.\n",
    "By “functional” here we mean “having no internal state”—in other words, “whose output value is solely and fully determined by the value input arguments.” Indeed, torch\n",
    ".nn.functional provides many functions that work like the modules we find in nn.\n",
    "But instead of working on the input arguments and stored parameters like the module counterparts, they take inputs and parameters as arguments to the function call.\n",
    "\n",
    "\n",
    "For instance, the functional counterpart of nn.Linear is nn.functional.linear,\n",
    "which is a function that has signature linear(input, weight, bias=None). The\n",
    "weight and bias parameters are arguments to the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can safely switch to the functional counterparts of pooling and activation, since they have no parameters:\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8*8*8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TIP:__ While general-purpose scientific functions like tanh still exist in\n",
    "torch.nn.functional in version 1.0, those entry points are deprecated in\n",
    "favor of functions in the top-level torch namespace. More niche functions\n",
    "like max_pool2d will remain in torch.nn.functional.\n",
    "\n",
    "Thus, the functional way also sheds light on what the nn.Module API is all about: a\n",
    "Module is a container for state in the forms of Parameters and submodules combined\n",
    "with the instructions to do a forward.\n",
    " \n",
    " \n",
    "Whether to use the functional or the modular API is a decision based on style and\n",
    "taste. When part of a network is so simple that we want to use nn.Sequential, we’re in\n",
    "the modular realm. When we are writing our own forwards, it may be more natural to\n",
    "use the functional interface for things that do not need state in the form of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In chapter 15, we will briefly touch on quantization. Then stateless bits like activations suddenly become stateful because information about the quantization needs to\n",
    "be captured. This means if we aim to quantize our model, it might be worthwhile to\n",
    "stick with the modular API if we go for non-JITed quantization. There is one style matter that will help you avoid surprises with (originally unforeseen) uses: \n",
    "\n",
    "- if you need several applications of stateless modules (like nn.HardTanh or nn.ReLU), it is probably a\n",
    "good idea to have a separate instance for each. Reusing the same module appears to\n",
    "be clever and will give correct results with our standard Python usage here, but tools\n",
    "analyzing your model may trip over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0858, -0.0521]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now lets run out model\n",
    "model = Net()\n",
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.4 Training our convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "        \n",
    "            train_loss += loss.item() #important to transform loss to Python # with .item(), to escape the gradients.\n",
    "            \n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(datetime.datetime.now(), \n",
    "                                                         epoch,\n",
    "                                                         train_loss / len(train_loader)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets put the dataset into our dataloader\n",
    "cifar2_loader = torch.utils.data.DataLoader(cifar2,batch_size=64, shuffle=True)\n",
    "cifar2_val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-16 12:51:40.654118 Epoch 0, Training loss 0.47706636786460876\n",
      "2020-09-16 12:51:45.990126 Epoch 1, Training loss 0.4424733519554138\n",
      "2020-09-16 12:52:34.272841 Epoch 10, Training loss 0.31934890151023865\n",
      "2020-09-16 12:53:28.479971 Epoch 20, Training loss 0.2900182604789734\n",
      "2020-09-16 12:54:23.978159 Epoch 30, Training loss 0.268887460231781\n",
      "2020-09-16 12:55:20.128728 Epoch 40, Training loss 0.2516702711582184\n",
      "2020-09-16 12:56:14.849697 Epoch 50, Training loss 0.23381511867046356\n",
      "2020-09-16 12:57:11.138372 Epoch 60, Training loss 0.21622593700885773\n",
      "2020-09-16 12:58:02.287637 Epoch 70, Training loss 0.205137237906456\n",
      "2020-09-16 12:58:53.539943 Epoch 80, Training loss 0.18898852169513702\n",
      "2020-09-16 12:59:47.260044 Epoch 90, Training loss 0.17661665380001068\n"
     ]
    }
   ],
   "source": [
    "training_loop(100, optimizer, model, loss_fn, cifar2_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4.1 Measuring accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, train_loader, val_loader):\n",
    "    for name, loader in  [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad(): #we dont want to keep track of gradients during eval\n",
    "            for imgs, labels in loader:\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                \n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "                \n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cast to a Python int—for integer tensors, this is equivalent to using .item(), similar to what we did in the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.93\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "validate_model(model, cifar2_loader, cifar2_val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4.2 Saving and loading our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), data_path + 'birds_vs_airplanes.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The birds_vs_airplanes.pt file now contains all the parameters of model: that is,\n",
    "weights and biases for the two convolution modules and the two linear modules. So, no structure—just the weights. This means when we deploy the model in production\n",
    "for our friend, we’ll need to keep the model class handy, create an instance, and then\n",
    "load the parameters back into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = Net()\n",
    "loaded_model.load_state_dict(torch.load(data_path + 'birds_vs_airplanes.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4.3 Training on the GPU\n",
    "\n",
    "We have a net and can train it! But it would be good to make it a bit faster. It is no surprise by now that we do so by moving our training onto the GPU. Using the .to\n",
    "method we saw in chapter 3, we can move the tensors we get from the data loader to\n",
    "the GPU, after which our computation will automatically take place there. But we also\n",
    "need to move our parameters to the GPU. Happily, nn.Module implements a .to function that moves all of its parameters to the GPU (or casts the type when you pass a\n",
    "dtype argument).\n",
    "\n",
    "There is a somewhat subtle difference between Module.to and Tensor.to.\n",
    "- Module.to is in place: the module instance is modified.\n",
    "\n",
    "- But Tensor.to is out of place\n",
    "(in some ways computation, just like Tensor.tanh), returning a new tensor\n",
    "\n",
    "\n",
    " One implication is that it is good practice to create the Optimizer after moving the parameters to the appropriate device\n",
    " \n",
    " It is considered good style to move things to the GPU if one is available. A good\n",
    "pattern is to set the a variable device depending on __torch.cuda.is_available__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-16 13:21:18.908422 Epoch 1, Training loss 0.0\n",
      "2020-09-16 13:22:07.677763 Epoch 10, Training loss 0.0\n",
      "2020-09-16 13:23:01.872507 Epoch 20, Training loss 0.0\n",
      "2020-09-16 13:23:54.519313 Epoch 30, Training loss 0.0\n",
      "2020-09-16 13:24:45.789868 Epoch 40, Training loss 0.0\n",
      "2020-09-16 13:25:37.162184 Epoch 50, Training loss 0.0\n",
      "2020-09-16 13:26:29.021354 Epoch 60, Training loss 0.0\n",
      "2020-09-16 13:27:22.109561 Epoch 70, Training loss 0.0\n",
      "2020-09-16 13:28:15.313390 Epoch 80, Training loss 0.0\n",
      "2020-09-16 13:29:06.121140 Epoch 90, Training loss 0.0\n",
      "2020-09-16 13:29:57.164484 Epoch 100, Training loss 0.0\n"
     ]
    }
   ],
   "source": [
    "#Now we change the loop by moving the tensors we get from the data loader to the GPU by using the Tensor.to method\n",
    "def training_loop(epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1,epochs+1):\n",
    "        train_loss = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            output = model(imgs)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            loss_train = loss.item()\n",
    "            \n",
    "            \n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                                                        datetime.datetime.now(), \n",
    "                                                        epoch,\n",
    "                                                        train_loss / len(train_loader)))\n",
    "\n",
    "                \n",
    "model = Net().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "training_loop(100, optimizer, model, loss_fn, cifar2_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our small network here, we do see a sizable increase in speed. The advantage\n",
    "of computing on GPUs is more visible for larger models.\n",
    " \n",
    " There is a slight complication when loading network weights: PyTorch will attempt\n",
    "to load the weight to the same device it was saved from—that is, weights on the GPU\n",
    "will be restored to the GPU. As we don’t know whether we want the same device, we\n",
    "have two options: we could move the network to the CPU before saving it, or move it\n",
    "back after restoring. It is a bit more concise to instruct PyTorch to override the device\n",
    "information when loading weights. This is done by passing the map_location keyword\n",
    "argument to torch.load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = Net().to(device=device)\n",
    "loaded_model.load_state_dict(torch.load(data_path + 'birds_vs_airplanes.pt',\n",
    "                                        map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.5 Model design\n",
    "\n",
    "We’ve reached the point where we can build a feed-forward convolutional\n",
    "neural network and train it successfully to classify images. The natural question is,\n",
    "what now? What if we are presented with a more complicated problem? \n",
    "\n",
    "\n",
    " If we moved to, say, ImageNet, we would find larger, more complex images, where\n",
    "the right answer would depend on multiple visual clues, often hierarchically organized. For instance, when trying to predict whether a dark brick shape is a remote control or a cell phone, the network could be looking for something like a screen. Plus images may not be our sole focus in the real world, where we have tabular data, sequences, and text. The promise of neural networks is sufficient flexibility to solve problems on all these kinds of data given the proper architecture (that is, the interconnection of layers or modules) and the proper loss function.\n",
    " \n",
    " \n",
    " PyTorch ships with a very comprehensive collection of modules and loss functions\n",
    "to implement state-of-the-art architectures ranging from feed-forward components to\n",
    "long short-term memory (LSTM) modules and transformer networks (two very popular architectures for sequential data). Several models are available through PyTorch\n",
    "Hub or as part of torchvision and other vertical community efforts.\n",
    "\n",
    "\n",
    "In general, it is beyond the scope of this\n",
    "book to explore variations on neural network architectures. However, we can build on\n",
    "the knowledge we’ve accumulated thus far to understand how we can implement almost any architecture thanks to the expressivity of PyTorch. The purpose of this\n",
    "section is precisely to provide conceptual tools that will allow us to read the latest\n",
    "research paper and start implementing it in PyTorch—or, since authors often release\n",
    "PyTorch implementations of their papers, to read the implementations without choking on our coffee.\n",
    "\n",
    "\n",
    "## 8.5.1 Adding memory capacity: Width\n",
    "\n",
    "Given our feed-forward architecture, there are a couple of dimensions we’d likely\n",
    "want to explore before getting into further complications.\n",
    "- The first dimension is the width of the network: the number of neurons per layer, or channels per convolution.\n",
    "\n",
    "\n",
    "We can make a model wider very easily in PyTorch. We just specify a larger number of\n",
    "output channels in the first convolution and increase the subsequent layers accordingly, taking care to change the forward function to reflect the fact that we’ll now\n",
    "have a longer vector once we switch to fully connected layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWidth(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(16 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 16 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to avoid hardcoding numbers in the definition of the model, we can easily\n",
    "pass a parameter to init and parameterize the width, taking care to also parameterize\n",
    "the call to view in the forward function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18090"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NetWidth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1,8*8* self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase the size of our model we increase its capacity, and the greater the capacity, the more variability in the inputs the model will be able to manage; but at the same time, the more likely overfitting will be, since the model can use a greater number of parameters to memorize unessential aspects of the input.\n",
    "\n",
    "## 8.5.2 Helping our model to converge and generalize: Regularization\n",
    "\n",
    "Training a model involves two critical steps: optimization, when we need the loss to\n",
    "decrease on the training set; and generalization, when the model has to work not only\n",
    "on the training set but also on data it has not seen before, like the validation set. The\n",
    "mathematical tools aimed at easing these two steps are sometimes subsumed under\n",
    "the label regularization\n",
    "\n",
    "__KEEPING THE PARAMETERS IN CHECK: WEIGHT PENALTIES__\n",
    "\n",
    "The first way to stabilize generalization is to add a regularization term to the loss. This\n",
    "term is crafted so that the weights of the model tend to be small on their own, limiting\n",
    "how much training makes them grow. In other words, it is a penalty on larger weight\n",
    "values. This makes the loss have a smoother topography, and there’s relatively less to\n",
    "gain from fitting individual samples.\n",
    "\n",
    "\n",
    " The most popular regularization terms of this kind are __L2 regularization, which is\n",
    "the sum of squares of all weights in the model__, and __L1 regularization, which is the sum\n",
    "of the absolute values of all weights in the model__\n",
    " Both of them are scaled by a\n",
    "(small) factor, which is a hyperparameter we set prior to training.\n",
    "\n",
    "\n",
    "__L2 regularization is also referred to as weight decay.__ The reason for this name is that,\n",
    "thinking about SGD and backpropagation, the negative gradient of the __L2 regularization term with respect to a parameter w_i__ is __- 2 * lambda * w_i,__ where lambda is the\n",
    "aforementioned hyperparameter, simply named weight decay in PyTorch. So, adding L2\n",
    "regularization to the loss function is equivalent to decreasing each weight by an\n",
    "amount proportional to its current value during the optimization step (hence, the\n",
    "name weight decay). Note that weight decay applies to all parameters of the network,\n",
    "such as biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn,\n",
    "train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss = 0\n",
    "        for imgs, labels in  train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            l2_lambda = 0.001\n",
    "            l2_norm = sum([p.pow(2.0).sum() for p in model.parameters()]) #L1 just replaces pow with abs\n",
    "            \n",
    "            loss = loss + l2_lambda * l2_norm\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                                                        datetime.datetime.now(),\n",
    "                                                        epoch,\n",
    "                                                        loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the SGD optimizer in PyTorch already has a weight_decay parameter that\n",
    "corresponds to 2 * lambda, and it directly performs weight decay during the update\n",
    "as described previously. It is fully equivalent to adding the L2 norm of weights to the\n",
    "loss, without the need for accumulating terms in the loss and involving autograd. \n",
    "\n",
    "__Therefore:__ Weight decay can also be passed to the optimizer\n",
    "\n",
    "__NOT RELYING TOO MUCH ON A SINGLE INPUT: DROPOUT__\n",
    "\n",
    "\n",
    "The idea behind dropout is indeed simple: zero out a random fraction of outputs from neurons across the network, where the randomization happens at each training iteration. This procedure effectively generates slightly different models with different neuron topologies at each iteration, giving neurons in the model less chance to coordinate in the memorization process that happens during overfitting. \n",
    "\n",
    "- An alternative point of view is that dropout perturbs the features being generated by the model, exerting an effect that is close to augmentation, but this time throughout the network.\n",
    "\n",
    "\n",
    "In PyTorch, we can implement dropout in a model by adding an __nn.Dropout module__ between the nonlinear activation function and the linear or convolutional module\n",
    "of the subsequent layer. As an argument, we need to specify the probability with which\n",
    "inputs will be zeroed out. In case of convolutions, we’ll use the specialized nn.Dropout2d or nn.Dropout3d, which zero out entire channels of the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_Dropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = nn.Dropout2d(p=0.4)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv2_dropout = nn.Dropout2d(p=0.4)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = self.conv1_dropout(out)\n",
    "        \n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2) \n",
    "        out = self.conv2_dropout(out)\n",
    "        \n",
    "        out = out.view(-1,8*8* self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that dropout is normally active during training, while during the evaluation of a\n",
    "trained model in production, dropout is bypassed or, equivalently, assigned a probability equal to zero. This is controlled through the train property of the Dropout\n",
    "module. Recall that PyTorch lets us switch between the two modalities by calling\n",
    "- __model.train()__ or  __model.eval()__ \n",
    "\n",
    "on any nn.Model subclass. The call will be automatically replicated on the submodules\n",
    "so that if Dropout is among them, it will behave accordingly in subsequent forward\n",
    "and backward passes.\n",
    "\n",
    "\n",
    "__KEEPING ACTIVATIONS IN CHECK: BATCH NORMALIZATION__\n",
    "\n",
    "The paper for Batch Normalization described a technique that had multiple beneficial effects on training: allowing us to increase the learning rate and make\n",
    "training less dependent on initialization and act as a regularizer, thus representing an alternative to dropout.\n",
    "\n",
    "The main idea behind batch normalization is to rescale the inputs to the activations of the network so that minibatches have a certain desirable distribution. Recalling the mechanics of learning and the role of nonlinear activation functions, this\n",
    "helps avoid the inputs to activation functions being too far into the saturated portion\n",
    "of the function, thereby killing gradients and slowing training\n",
    "\n",
    "In practical terms, batch normalization shifts and scales an intermediate input\n",
    "using the mean and standard deviation collected at that intermediate location over\n",
    "the samples of the minibatch. The regularization effect is a result of the fact that an\n",
    "individual sample and its downstream activations are always seen by the model as\n",
    "shifted and scaled, depending on the statistics across the randomly extracted minibatch. This is in itself a form of principled augmentation. The authors of the paper\n",
    "suggest that using batch normalization eliminates or at least alleviates the need\n",
    "for dropout.\n",
    "\n",
    "Batch normalization in PyTorch is provided through the nn.BatchNorm1D,\n",
    "nn.BatchNorm2d, and nn.BatchNorm3d modules, depending on the dimensionality of\n",
    "the input. Since the aim for batch normalization is to rescale the inputs of the activations, the natural location is after the linear transformation (convolution, in this case)\n",
    "and the activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBatchNorm(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_batchnorm = nn.BatchNorm2d(num_features=n_chans1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv2_batchnorm = nn.BatchNorm2d(num_features=n_chans1 // 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1_batchnorm(self.conv1(x))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        \n",
    "        out = self.conv2_batchnorm(self.conv2(out))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        \n",
    "        out = out.view(-1,8*8* self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as for dropout, batch normalization needs to behave differently during training\n",
    "and inference. In fact, at inference time, we want to avoid having the output for a specific input depend on the statistics of the other inputs we’re presenting to the model.\n",
    "As such, we need a way to still normalize, but this time fixing the normalization\n",
    "parameters once and for all.\n",
    " As minibatches are processed, in addition to estimating the mean and standard\n",
    "deviation for the current minibatch, PyTorch also updates the running estimates for\n",
    "mean and standard deviation that are representative of the whole dataset, as an\n",
    "approximation. This way, when the user specifies\n",
    "- model.eval()\n",
    "\n",
    "and the model contains a batch normalization module, the running estimates are frozen and used for normalization. To unfreeze running estimates and return to using\n",
    "the minibatch statistics, we call model.train(), just as we did for dropout. \n",
    "\n",
    "## 8.5.3 Going deeper to learn more complex structures: Depth__\n",
    "\n",
    " With depth, the complexity of the function the network is able to approximate generally increases. In regard to computer vision, a shallower network could\n",
    "identify a person’s shape in a photo, whereas a deeper network could identify the person, the face on their top half, and the mouth within the face. Depth allows a model\n",
    "to deal with hierarchical information when we need to understand the context in\n",
    "order to say something about some input.\n",
    "\n",
    "__SKIP CONNECTIONS__\n",
    "Depth comes with some additional challenges, which prevented deep learning models\n",
    "from reaching 20 or more layers until late 2015. Adding depth to a model generally\n",
    "makes training harder to converge. Let’s recall backpropagation and think about it in the context of a very deep network\n",
    "\n",
    "\n",
    "The derivatives of the loss function with respect to\n",
    "the parameters, especially those in early layers, need to be multiplied by a lot of other\n",
    "numbers originating from the chain of derivative operations between the loss and the\n",
    "parameter. Those numbers being multiplied could be small, generating ever-smaller\n",
    "numbers, or large, swallowing smaller numbers due to floating-point approximation.\n",
    "The bottom line is that a long chain of multiplications will tend to make the contribution of the parameter to the gradient __vanish__, leading to ineffective training of that layer\n",
    "since that parameter and others like it won’t be properly updated.\n",
    "\n",
    "\n",
    " In December 2015, Kaiming He and coauthors presented residual networks (ResNets), an architecture that uses a simple trick to allow very deep networks to be successfully trained That work opened the door to networks ranging from tens of layers to 100 layers in depth, surpassing the then state of the art in computer vision benchmark problems. We encountered residual networks when we were playing with pretrained models in chapter 2. The trick we mentioned is the following: using a skip connection to short-circuit blocks of layers.\n",
    " \n",
    " \n",
    " A skip connection is nothing but the addition of the input to the output of a block of layers. This is exactly how it is done in PyTorch. Let’s add one layer to our simple convolutional model, and let’s use ReLU as the activation for a change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets look at the vanilla model first then compare to a ResNet\n",
    "\n",
    "class NetDepth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)), 2)\n",
    "        \n",
    "        out = out.view(-1,4*4* self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a skip connection a la ResNet to this model amounts to adding the output of\n",
    "the first layer in the forward function to the input of the third layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetRes(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out1 = out #Here we save the previous layer so we can add it to conv3 (i.e a skip connection)\n",
    "        \n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2) #add previous layer out1 to this layers output\n",
    "        \n",
    "        out = out.view(-1, 4 * 4 * n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, we’re using the output of the first activations as inputs to the last, in\n",
    "addition to the standard feed-forward path. This is also referred to as __identity mapping.__\n",
    "\n",
    "\n",
    "So, how does this alleviate the issues with vanishing gradients we were mentioning\n",
    "earlier?\n",
    "\n",
    " Thinking about backpropagation, we can appreciate that a skip connection, or a\n",
    "sequence of skip connections in a deep network, creates a direct path from the deeper\n",
    "parameters to the loss. This makes their contribution to the gradient of the loss more\n",
    "direct, as partial derivatives of the loss with respect to those parameters have a chance\n",
    "not to be multiplied by a long chain of other operations.\n",
    "\n",
    "\n",
    " It has been observed that skip connections have a beneficial effect on convergence\n",
    "especially in the initial phases of training. Also, the loss landscape of deep residual\n",
    "networks is a lot smoother than feed-forward networks of the same depth and width.\n",
    " It is worth noting that skip connections were not new to the world when ResNets\n",
    "came along. Highway networks and U-Net made use of skip connections of one form or another. However, the way ResNets used skip connections enabled models of\n",
    "depths greater than 100 to be amenable to training.\n",
    " \n",
    " \n",
    " Since the advent of ResNets, other architectures have taken skip connections to\n",
    "the next level. One in particular, __DenseNet__, proposed to connect each layer with several other layers downstream through skip connections, achieving state-of-the-art\n",
    "results with fewer parameters. By now, we know how to implement something like\n",
    "DenseNets: just arithmetically add earlier intermediate outputs to downstream intermediate outputs. \n",
    "\n",
    "\n",
    "__BUILDING VERY DEEP MODELS IN PYTORCH__\n",
    "\n",
    "We talked about exceeding 100 layers in a convolutional neural network. How can we\n",
    "build that network in PyTorch without losing our minds in the process?\n",
    "\n",
    "The standard strategy is to define a __building block__, such as a __(Conv2d, ReLU, Conv2d) + skip\n",
    "connection block__, and __then build the network dynamically in a for loop__.\n",
    "\n",
    "\n",
    "We first create a __module subclass__ whose sole job is to provide the computation for one\n",
    "block—that is, one group of convolutions, activation, and skip connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3, \n",
    "                              padding=1, bias=False) # Batch norm cancels out effects of Bias so we leave it out\n",
    "        self.batch_norm = nn.BatchNorm2d(n_chans)\n",
    "        \n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu') \n",
    "        \n",
    "#Uses custom initializations\n",
    "#.kaiming_normal_ initializes with normal random elements with std as computed in the ResNet paper.\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5 )\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "#The batch norm is initialized to produce output distributions that initially have 0 mean and 0.5 variance\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        \n",
    "        return out + x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32, 32])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = ResBlock(n_chans=3)\n",
    "block(img.unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First, in init, we create nn.Sequential containing a list of ResBlock instances.\n",
    "nn.Sequential will ensure that the output of one block is used as input to the next. It\n",
    "will also ensure that all the parameters in the block are visible to Net. Then, in forward,\n",
    "we just call the sequential to traverse the 100 blocks and generate the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetResDeep(nn.Module):\n",
    "    def __init__(self,n_chans1=32,n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.resblocks = nn.Sequential(\n",
    "                            *(num_blocks * [ResBlock(n_chans=n_chans1)])\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "            out = self.resblocks(out)\n",
    "            out = out.view(-1, 8 * 8 * n_chans1)\n",
    "            out = torch.relu(self.fc1(out))\n",
    "            out = self.fc2(out)\n",
    "            \n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the implementation, we parameterize the actual number of layers, which is important for experimentation and reuse. Also, needless to say, backpropagation will work as\n",
    "expected. Unsurprisingly, the network is quite a bit slower to converge. It is also more fragile in convergence. This is why we used more-detailed initializations and trained\n",
    "our NetRes with a learning rate of 3e – 3 instead of the 1e – 2 we used for the other\n",
    "networks. \n",
    "\n",
    "(Obviously a lot more work and tuning is needed to converge a 100 layer network on a complicated dataset)\n",
    "\n",
    "__INITIALIZATION__\n",
    "\n",
    "Initialization is one of the\n",
    "important tricks in training neural networks. Unfortunately, for historical reasons,\n",
    "PyTorch has default weight initializations that are not ideal. People are looking at fixing the situation.\n",
    "\n",
    "\n",
    " In the meantime, we need to fix the\n",
    "weight initialization ourselves. We found that our model did not converge and looked\n",
    "at what people commonly choose as initialization (a smaller variance in weights; and\n",
    "zero mean and unit variance outputs for batch norm), and then we halved the output\n",
    "variance in the batch norm when the network would not converge.\n",
    "\n",
    " Weight initialization could fill an entire chapter on its own, but we think that\n",
    "would be excessive. In chapter 11, we’ll bump into initialization again and use what\n",
    "arguably could be PyTorch defaults without much explanation. Once you’ve progressed to the point where the details of weight initialization are of specific interest to\n",
    "you—probably not before finishing this book—you might revisit this topic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:facial_rec]",
   "language": "python",
   "name": "conda-env-facial_rec-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
