{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Floating Point Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Multidimensional Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1.0,2.0,1.0]\n",
    "#lists are mutable, tensors are able to be changed the same way\n",
    "a[2] = 3.0\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now tensors\n",
    "a = torch.ones(3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 2.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2] = 2.0\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can access an element using its zero-based\n",
    "index or assign a new value to it. Although on the surface this example doesn’t differ\n",
    "much from a list of number objects, under the hood things are completely different. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python lists or tuples of numbers are collections of Python objects that are individually\n",
    "allocated in memory, as shown on the left in figure 3.3. PyTorch tensors or NumPy\n",
    "arrays, on the other hand, are views over (typically) contiguous memory blocks containing unboxed C numeric types rather than Python objects. Each element is a 32-bit (4-byte)\n",
    "float in this case, as we can see on the right side of figure 3.3. This means storing a 1D\n",
    "tensor of 1,000,000 float numbers will require exactly 4,000,000 contiguous bytes, plus\n",
    "a small overhead for the metadata (such as dimensions and numeric type)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Say we have a list of coordinates we’d like to use to represent a geometrical object:\n",
    "perhaps a 2D triangle with vertices at coordinates (4, 1), (5, 3), and (2, 1). The\n",
    "example is not particularly pertinent to deep learning, but it’s easy to follow. Instead\n",
    "of having coordinates as numbers in a Python list, as we did earlier, we can use a one-dimensional tensor by storing Xs in the even indices and Ys in the odd indices,\n",
    "like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.zeros(6)\n",
    "points[0] = 4.0\n",
    "points[1] = 1.0\n",
    "points[2] = 5.0\n",
    "points[3] = 3.0\n",
    "points[4] = 2.0\n",
    "points[5] = 1.0\n",
    "\n",
    "# or\n",
    "points = torch.tensor([4.0,1.0,5.0,3.0,2.0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.0, 1.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To get the coordinates of the first point, we do the following:\n",
    "float(points[0]), float(points[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is OK, although it would be practical to have the first index refer to individual 2D\n",
    "points rather than point coordinates. For this, we can use a 2D tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4., 1.],\n",
       "         [5., 3.],\n",
       "         [2., 1.]]),\n",
       " torch.Size([3, 2]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points,points.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This informs us about the size of the tensor along each dimension. We could also use\n",
    "zeros or ones to initialize the tensor, providing the size as a tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.zeros(3,2)\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4., 1.],\n",
       "         [5., 3.],\n",
       "         [2., 1.]]),\n",
       " tensor(1.),\n",
       " tensor([4., 1.]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we can access an individual element in the tensor using one or two indices:\n",
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points, points[0,1], points[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is another tensor that presents a different view of the same underlying data.\n",
    "The new tensor is a 1D tensor of size 2, referencing the values of the first row in the\n",
    "points tensor. Does this mean a new chunk of memory was allocated, values were copied\n",
    "into it, and the new memory was returned wrapped in a new tensor object? No, because\n",
    "that would be very inefficient, especially if we had millions of points. We’ll revisit how\n",
    "tensors are stored later in this chapter when we cover views of tensors in section 3.7. \n",
    "\n",
    "# 3.3 Indexing tensors\n",
    " can index a tensor and grab either in any dimension points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grabs all rows after the first\n",
    "points[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 2.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab first column from all rows but first\n",
    "points[1:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 1.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grabs first row and all columns\n",
    "points[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4., 1.],\n",
       "         [5., 3.],\n",
       "         [2., 1.]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adds dimension of size 1, just like unsqueeze\n",
    "points[None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to using ranges, PyTorch features a powerful form of indexing, called\n",
    "advanced indexing, which we will look at in the next chapter. \n",
    "\n",
    "\n",
    "# 3.4 Named tensors\n",
    "\n",
    "<b>Note: Named tensors are still experimental and should not be used in anything put into productions</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensions (or axes) of our tensors usually index something like pixel locations\n",
    "or color channels. This means when we want to index into a tensor, we need to\n",
    "remember the ordering of the dimensions and write our indexing accordingly. As\n",
    "data is transformed through multiple tensors, keeping track of which dimension contains what data can be error-prone.\n",
    "\n",
    " To make things concrete, imagine that we have a 3D tensor like img_t from section\n",
    "2.1.4 (we will use dummy data for simplicity here), and we want to convert it to grayscale. We looked up typical weights for the colors to derive a single brightness value:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = torch.randn(3,5,5) # shape [channel,rows,columns]\n",
    "weights = torch.tensor([0.2126,0.7152,0.0722])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also often want our code to generalize—for example, from grayscale images represented as 2D tensors with height and width dimensions to color images adding a third\n",
    "channel dimension (as in RGB), or from a single image to a batch of images. In section 2.1.4, we introduced an additional batch dimension in batch_t; here we pretend\n",
    "to have a batch of 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_t = torch.randn(2,3,5,5) # shape [batch, channel,rows, columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So sometimes the RGB channels are in dimension 0, and sometimes they are in dimension 1. But we can generalize by counting from the end: they are always in dimension\n",
    "–3, the third from the end. The lazy, unweighted mean can thus be written as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_naive = img_t.mean(-3)\n",
    "batch_gray_naive = batch_t.mean(-3)\n",
    "img_gray_naive.shape, batch_gray_naive.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now we have the weight, too. PyTorch will allow us to multiply things that are the\n",
    "same shape, as well as shapes where one operand is of size 1 in a given dimension. It\n",
    "also appends leading dimensions of size 1 automatically. This is a feature called broadcasting. batch_t of shape (2, 3, 5, 5) is multiplied by unsqueezed_weights of shape (3,\n",
    "1, 1), resulting in a tensor of shape (2, 3, 5, 5), from which we can then sum the third\n",
    "dimension from the end (the three channels):\n",
    "\n",
    "Note: This is whats usually done behind the scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 5, 5]), torch.Size([2, 3, 5, 5]), torch.Size([3, 1, 1]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsqueezed_weights = weights.unsqueeze(-1).unsqueeze_(-1)\n",
    "\n",
    "img_weights = (img_t * unsqueezed_weights)\n",
    "\n",
    "batch_weights = (batch_t * unsqueezed_weights)\n",
    "\n",
    "img_gray_weighted = img_weights.sum(-3)\n",
    "\n",
    "batch_gray_weighted = batch_weights.sum(-3)\n",
    "\n",
    "batch_weights.shape, batch_weights.shape, unsqueezed_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this gets messy quickly—and for the sake of efficiency—the PyTorch function\n",
    "einsum (adapted from NumPy) specifies an indexing mini-language2\n",
    " giving index\n",
    "names to dimensions for sums of such products. As often in Python, broadcasting—a\n",
    "form of summarizing unnamed things—is done using three dots '…'; but don’t worry\n",
    "too much about einsum, because we will not use it in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 5])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted_fancy = torch.einsum('...chw,c->...hw', img_t, weights)\n",
    "batch_gray_weighted_fancy = torch.einsum('...chw,c->...hw', batch_t, weights)\n",
    "batch_gray_weighted_fancy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is quite a lot of bookkeeping involved. This is error-prone, especially when the locations where tensors are created and used are far apart in our code.\n",
    "This has caught the eye of practitioners, and so it has been suggested3\n",
    " that the dimension be given a name instead.\n",
    " \n",
    " \n",
    " PyTorch 1.3 added named tensors as an experimental feature (see https://pytorch.org/tutorials/intermediate/named_tensor_tutorial.html and https://pytorch.org/docs/stable/named_tensor.html). Tensor factory functions such as tensor and rand\n",
    "take a names argument. The names should be a sequence of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/pip-req-build-sqyrlut3/c10/core/TensorImpl.h:845: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7165, 0.0722], names=('channels',))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_named = torch.tensor([0.2126, 0.71652, 0.0722], names=['channels'])\n",
    "weights_named"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we already have a tensor and want to add names (but not change existing\n",
    "ones), we can call the method refine_names on it. Similar to indexing, the ellipsis (…)\n",
    "allows you to leave out any number of dimensions. With the rename sibling method,\n",
    "you can also overwrite or drop (by passing in None) existing names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img named: torch.Size([3, 5, 5]) ('channels', 'rows', 'columns')\n",
      "batch named: torch.Size([2, 3, 5, 5]) (None, 'channels', 'rows', 'columns')\n"
     ]
    }
   ],
   "source": [
    "img_named = img_t.refine_names(..., 'channels', 'rows', 'columns')\n",
    "batch_named = batch_t.refine_names(..., 'channels', 'rows', 'columns')\n",
    "print('img named:', img_named.shape, img_named.names)\n",
    "print('batch named:', batch_named.shape, batch_named.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For operations with two inputs, in addition to the usual dimension checks—whether\n",
    "sizes are the same, or if one is 1 and can be broadcast to the other—PyTorch will now\n",
    "check the names for us. So far, it does not automatically align dimensions, so we need\n",
    "to do this explicitly. The method align_as returns a tensor with missing dimensions\n",
    "added and existing ones permuted to the right order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 1]), ('channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_aligned = weights_named.align_as(img_named)\n",
    "weights_aligned.shape, weights_aligned.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), ('rows', 'columns'))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Functions accepting dimension arguments, like sum, also take named dimensions:\n",
    "gray_named = (img_named * weights_aligned).sum('channels')\n",
    "gray_named.shape, gray_named.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error when attempting to broadcast dims ['channels', 'rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-fad89d397f3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#If we try to combine dimensions with different names, we get an error:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgray_named\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_named\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweights_named\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'channels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Error when attempting to broadcast dims ['channels', 'rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match."
     ]
    }
   ],
   "source": [
    "#If we try to combine dimensions with different names, we get an error:\n",
    "gray_named = (img_named[..., :3] * weights_named).sum('channels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use tensors outside functions that operate on named tensors, we need to\n",
    "drop the names by renaming them to None. The following gets us back into the world\n",
    "of unnamed dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), (None, None))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_plain = gray_named.rename(None)\n",
    "gray_plain.shape, gray_plain.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the experimental nature of this feature at the time of writing, and to avoid\n",
    "mucking around with indexing and alignment, we will stick to unnamed in the\n",
    "remainder of the book. Named tensors have the potential to eliminate many sources\n",
    "of alignment errors, which—if the PyTorch forum is any indication—can be a source\n",
    "of headaches. It will be interesting to see how widely they will be adopted. \n",
    "\n",
    "\n",
    "# 3.5 Tensor Element Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor element types\n",
    "So far, we have covered the basics of how tensors work, but we have not yet touched on\n",
    "what kinds of numeric types we can store in a Tensor. As we hinted at in section 3.2,\n",
    "using the standard Python numeric types can be suboptimal for several reasons:\n",
    " - Numbers in Python are objects. Whereas a floating-point number might require\n",
    "only, for instance, 32 bits to be represented on a computer, Python will convert\n",
    "it into a full-fledged Python object with reference counting, and so on. This\n",
    "operation, called boxing, is not a problem if we need to store a small number of\n",
    "numbers, but allocating millions gets very inefficient.\n",
    "\n",
    "-  Lists in Python are meant for sequential collections of objects. There are no operations\n",
    "defined for, say, efficiently taking the dot product of two vectors, or summing vectors together. Also, Python lists have no way of optimizing the layout of their contents in memory, as they are indexable collections of pointers to Python objects (of any kind, not just numbers). Finally, Python lists are one-dimensional, and\n",
    "although we can create lists of lists, this is again very inefficient.\n",
    "\n",
    "\n",
    "-  The Python interpreter is slow compared to optimized, compiled code. Performing mathematical operations on large collections of numerical data can be much faster\n",
    "using optimized code written in a compiled, low-level language like C.\n",
    "For these reasons, data science libraries rely on NumPy or introduce dedicated data\n",
    "structures like PyTorch tensors, which provide efficient low-level implementations of\n",
    "numerical data structures and related operations on them, wrapped in a convenient\n",
    "high-level API. To enable this, the objects within a tensor must all be numbers of the\n",
    "same type, and PyTorch must keep track of this numeric type.\n",
    "\n",
    "\n",
    "## 3.5.1Specifying the numeric type with dtype\n",
    "\n",
    "The dtype argument to tensor constructors (that is, functions like tensor, zeros, and\n",
    "ones) specifies the numerical data (d) type that will be contained in the tensor. The\n",
    "data type specifies the possible values the tensor can hold (integers versus floatingpoint numbers) and the number of bytes per value.4\n",
    " The dtype argument is deliberately similar to the standard NumPy argument of the same name\n",
    " \n",
    " \n",
    " The default data type for tensors is 32-bit floating-point. \n",
    " \n",
    " ## 3.5.2 A dtype for every occasion\n",
    "As we will see in future chapters, computations happening in neural networks are typically executed with 32-bit floating-point precision. Higher precision, like 64-bit, will\n",
    "not buy improvements in the accuracy of a model and will require more memory and\n",
    "computing time. The 16-bit floating-point, half-precision data type is not present\n",
    "natively in standard CPUs, but it is offered on modern GPUs. It is possible to switch to\n",
    "half-precision to decrease the footprint of a neural network model if needed, with a\n",
    "minor impact on accuracy.\n",
    "\n",
    " Tensors can be used as indexes in other tensors. In this case, PyTorch expects\n",
    "indexing tensors to have a 64-bit integer data type. Creating a tensor with integers as\n",
    "arguments, such as using torch.tensor([2, 2]), will create a 64-bit integer tensor by\n",
    "default. As such, we’ll spend most of our time dealing with float32 and int64.\n",
    " \n",
    " Finally, predicates on tensors, such as points > 1.0, produce bool tensors indicating whether each individual element satisfies the condition. These are the numeric\n",
    "types in a nutshell. \n",
    "\n",
    "## 3.5.3 Managing a tensor’s dtype attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float64, torch.int16)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can specify the dtype as an argument when creating a tensor\n",
    "double_points = torch.ones(10,2, dtype=torch.double)\n",
    "short_points = torch.ones(10,2, dtype=torch.short)\n",
    "double_points.dtype, short_points.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can also cast dtype using the following syntax:\n",
    "double_points = torch.ones(10,2).double()\n",
    "short_points = torch.ones(10,2).short()\n",
    "\n",
    "#or more convenient 'to' method\n",
    "double_points = torch.ones(10,2).to(torch.double)\n",
    "short_points = torch.ones(10,2).to(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>to</b> checks whether the conversion is necessary, and if so, performs the conversion. Dtype casting methods like float are shorthands for <b>to</b>, but <b>to</b> allows us to use additional arguments. When mixing dtypes in operations, the tensors will go to the LARGER dtype by default, so need to transform all tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, -0.0000, -0.0000, 1.1781], dtype=torch.float64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_64 = torch.randn(5, dtype=torch.double)\n",
    "points_short = points_64.to(torch.short)\n",
    "points_64*points_short #gives us a float64 tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6 The Tensor API\n",
    "\n",
    "Vast majority of operations on abd between tensors are available in the torch module and can also be called as methods of a tensor object:\n",
    "\n",
    "EX: Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]), torch.Size([2, 3]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3,2)\n",
    "a_t = torch.transpose(a,0,1)\n",
    "a.shape, a_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]), torch.Size([2, 3]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#or as a method of a tensor\n",
    "a = torch.ones(3,2)\n",
    "a_t = a.transpose(0,1)\n",
    "a.shape, a_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  the tensor operations divided into groups:\n",
    "\n",
    "- Creation ops—Functions for constructing a tensor, like ones and from_numpy\n",
    "- Indexing, slicing, joining, mutating ops—Functions for changing the shape, stride, or content of a tensor, like transpose\n",
    "-  Math ops—Functions for manipulating the content of the tensor through\n",
    "computations\n",
    "    - Pointwise ops—Functions for obtaining a new tensor by applying a function to\n",
    "each element independently, like abs and cos\n",
    "    - Reduction ops—Functions for computing aggregate values by iterating through tensors, like mean, std, and norm\n",
    "    - Comparison ops—Functions for evaluating numerical predicates over tensors, like equal and max\n",
    "    - Spectral ops—Functions for transforming in and operating in the frequency domain, like stft and hamming_window\n",
    "    - Other operations—Special functions operating on vectors, like cross, or matrices, like trace\n",
    "    - BLAS and LAPACK operations—Functions following the Basic Linear AlgebraSubprograms (BLAS) specification for scalar, vector-vector, matrix-vector, and matrix-matrix operations\n",
    "\n",
    "- Random sampling—Functions for generating values by drawing randomly from probability distributions, like randn and normal\n",
    "- Serialization—Functions for saving and loading tensors, like load and save\n",
    "- Parallelism—Functions for controlling the number of threads for parallel CPU execution, like set_num_threads\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# 3.7 Tensors: Scenic Views of Storage\n",
    "\n",
    "It is time for us to look a bit closer at the implementation under the hood. Values in\n",
    "tensors are allocated in contiguous chunks of memory managed by torch.Storage\n",
    "instances. A storage is a one-dimensional array of numerical data: that is, a contiguous\n",
    "block of memory containing numbers of a given type, such as float (32 bits representing a floating-point number) or int64 (64 bits representing an integer). A\n",
    "PyTorch Tensor instance is a view of such a Storage instance that is capable of indexing into that storage using an offset and per-dimension strides. \n",
    "\n",
    "Multiple tensors can index the same storage even if they index into the data differently. Previously when we requested <b>points[0]</b>, what we got back was another tensor that <i>\"views\"</i> the same storage the <b>points</b> tensor uses. \n",
    "\n",
    "\n",
    "## 3.7.1 Indexing into Storage\n",
    "The storage for a given tensor is accessible using the <b>.storage</b> property:\n",
    "\n",
    "\n",
    "Storage may not be directly accessible in future PyTorch releases, but what we show here still provides a good\n",
    "mental picture of how tensors work under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 4.0\n",
       " 1.0\n",
       " 5.0\n",
       " 3.0\n",
       " 2.0\n",
       " 1.0\n",
       "[torch.FloatStorage of size 6]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0,1.0],[5.0,3.0],[2.0,1.0]])\n",
    "points.storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the tensor reports itself as having three rows and two columns, the storage under the hood is a contiguous array of size 6. We can also index into a storage manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_storage = points.storage()\n",
    "points_storage[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can’t index a storage of a 2D tensor using two indices. The layout of a storage is\n",
    "always one-dimensional, regardless of the dimensionality of any and all tensors that\n",
    "might refer to it.\n",
    "\n",
    " At this point, it shouldn’t come as a surprise that changing the value of a storage\n",
    "leads to changing the content of its referring tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0,1.0],[5.0,3.0],[2.0,1.0]])\n",
    "points_storage = points.storage()\n",
    "points_storage[0] = 2\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7.2 Modifying Stored Values: In-place Operations\n",
    "\n",
    "In addition to the operations on tensors introduced in the previous section, a small\n",
    "number of operations exist only as methods of the Tensor object. They are recognizable from a trailing underscore in their name, like zero_, which indicates that the\n",
    "method operates in place by modifying the input instead of creating a new output tensor\n",
    "and returning it\n",
    "\n",
    "Any method without the trailing underscore leaves the source tensor unchanged and\n",
    "instead returns a new tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3,2)\n",
    "a.zero_()\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.8 Tensor Metadata: Size, Offset, Stride\n",
    "\n",
    "Tensors rely on a few pieces of information to index into storage: Size, Offset, Stride\n",
    "\n",
    "- The size (or shape, in NumPy parlance) is a tuple indicating how many elements across each dimension the tensor represents. \n",
    "- The storage offset is the index in the storage corresponding to the first element in the tensor.\n",
    "- The stride is the number of elements in the storage that need to be skipped over to\n",
    "obtain the next element along each dimension\n",
    "\n",
    "## 3.8.1 Views of another tensor’s storage\n",
    "\n",
    "We can get the second point in the tensor by providing the corresponding index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([2]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "second_point = points[1]\n",
    "second_point.storage_offset(), second_point.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting tensor has offset 2 in the storage (since we need to skip the first point,\n",
    "which has two items), and the size is an instance of the Size class containing one element, since the tensor is one-dimensional. It’s important to note that this is the\n",
    "same information contained in the shape property of tensor objects\n",
    "\n",
    "\n",
    "The stride is a tuple indicating the number of elements in the storage that have to be skipped when the index is increased by 1 in each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2]), (2, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_point.shape, points.stride()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indirection between Tensor and Storage makes some operations inexpensive, like transposing a tensor or extracting a subtensor, because they do not lead to\n",
    "memory reallocations. Instead, they consist of allocating a new Tensor object with a\n",
    "different value for size, storage offset, or stride.\n",
    "\n",
    "\n",
    "Since new tensors are just views of the original tensor, anytime we change the new tensor, the original is also ammended. To avoid this we can clone the tensors to create a new one in storage insted of a view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "second_point = points[1].clone()\n",
    "second_point[0] = 10.0\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.9 Moving Tensors to the GPU\n",
    "\n",
    "Every PyTorch tensor can be transferred to (one of) the\n",
    "GPU(s) in order to perform massively parallel, fast computations. All operations that\n",
    "will be performed on the tensor will be carried out using GPU-specific routines that\n",
    "come with PyTorch.\n",
    "\n",
    "In addition to dtype, a PyTorch Tensor also has the notion of device, which is where\n",
    "on the computer the tensor data is placed. Here is how we can create a tensor on the\n",
    "GPU by specifying the corresponding argument to the constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checks to see if a cuda device is enable\n",
    "torch.cuda.is_available()\n",
    "\n",
    "#if CUDA device is enabled you can put tensors on the GPU using the following\n",
    "points_gpu = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], device='cuda')\n",
    "\n",
    "\n",
    "#We could instead copy a tensor created on the CPU onto the GPU using the \"to\" method:\n",
    "points_gpu = points.to(device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing so returns a new tensor that has the same numerical data, but stored in the\n",
    "RAM of the GPU, rather than in regular system RAM. Now that the data is stored\n",
    "locally on the GPU, we’ll start to see the speedups mentioned earlier when performing mathematical operations on the tensor. In almost all cases, CPU- and GPU-based\n",
    "tensors expose the same user-facing API, making it much easier to write code that is\n",
    "agnostic to where, exactly, the heavy number crunching is running.\n",
    " \n",
    " \n",
    " If our machine has more than one GPU, we can also decide on which GPU we allocate the tensor by passing a zero-based integer identifying the GPU on the machine,\n",
    "such as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_gpu = points.to(device='cuda:0')\n",
    "\n",
    "#At this point, any operation performed on the tensor, is carried out on the GPU\n",
    "\n",
    "points = 2 * points  #performed and stays on CPU\n",
    "points_gpu=2* points.to(device='cuda') #performs and stays on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the points_gpu tensor is not brought back to the CPU once the result has\n",
    "been computed. Here’s what happened in this line:\n",
    "\n",
    "1. The points tensor is copied to the GPU.\n",
    "2. A new tensor is allocated on the GPU and used to store the result of the multiplication.\n",
    "3. A handle to that GPU tensor is returned.\n",
    "\n",
    "Therefore, if we also add a constant to points_gpu that addition is still performed on the GPU\n",
    "\n",
    "We can also use the shorthand methods cpu and cuda instead of the to method to\n",
    "achieve the same goal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_gpu = points.cuda()\n",
    "points_gpu = points.cuda(0)\n",
    "points_cpu = points_gpu.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.10 NumPy Interoperability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facial_rec",
   "language": "python",
   "name": "facial_rec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
